# Model
model_size:
  small:
    context_size: 256
    n_layer: 2
    n_head: 2
    d_embed: 128
    d_ff: 512
    dropout: 0.1
    flash_attention: False
  medium:
    context_size: 256
    n_layer: 6
    n_head: 6
    d_embed: 384
    d_ff: 1536
    dropout: 0.2
    flash_attention: False
  large:
    context_size: 256
    n_layer: 12
    n_head: 12
    d_embed: 768
    d_ff: 3072
    dropout: 0.2
    flash_attention: False
  xl:
    context_size: 256
    n_layer: 24
    n_head: 24
    d_embed: 1024
    d_ff: 4096
    dropout: 0.2
    flash_attention: False

# Training
dataset_size: 0.01, 0.05, 0.1, 0.5, 1.0
epochs: 1
batch_size: 64
optimizer:
  name: AdamW  # AdamW or SGD
  params:
    lr: 0.001
    weight_decay: 0.01
scheduler:
  type: cosine  # linear or cosine
  warmup_ratio: 0.1
grad_clip: 1.0
mixed_precision: False
seed: 101

dataset: shakespeare  # shakespeare or openweb
val_size: 0.1

# WandB
project: LLM101-Scaling-Laws