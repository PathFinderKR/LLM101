# Model
gpt:
  small:
    context_size: 256
    n_layer: 2
    n_head: 2
    d_embed: 128
    d_ff: 512
    dropout: 0.1
    flash_attention: False
  medium:
    context_size: 256
    n_layer: 4
    n_head: 4
    d_embed: 256
    d_ff: 1024
    dropout: 0.2
    flash_attention: False
  large:
    context_size: 256
    n_layer: 6
    n_head: 6
    d_embed: 384
    d_ff: 1536
    dropout: 0.2
    flash_attention: False
  xl:
    context_size: 256
    n_layer: 8
    n_head: 8
    d_embed: 512
    d_ff: 2048
    dropout: 0.3
    flash_attention: False

mlp:
  small:
    context_size: 256
    d_embed: 128
    d_ff: 512
    dropout: 0.1
  medium:
    context_size: 256
    d_embed: 256
    d_ff: 1024
    dropout: 0.2
  large:
    context_size: 256
    d_embed: 384
    d_ff: 1536
    dropout: 0.2

# Training
dataset_size:
  xs: 0.01
  small: 0.02
  medium: 0.03
  large: 0.05
  xl: 0.1

epochs: 1  # Fixed
batch_size: 128  # Fixed
optimizer:
  name: AdamW  # AdamW or SGD
  params:
    lr: 0.0002
    weight_decay: 0.1
scheduler:
  type: cosine  # linear or cosine
  warmup_ratio: 0.1
grad_clip: 1.0
mixed_precision: False
seed: 101

dataset: shakespeare  # shakespeare or openweb
val_size: 0.1