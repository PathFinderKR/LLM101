# Model
model_size:
  small:
    context_size: 256
    n_layer: 2
    n_head: 2
    d_embed: 128
    d_ff: 512
    dropout: 0.1
    flash_attention: False
  medium:
    context_size: 256
    n_layer: 6
    n_head: 6
    d_embed: 384
    d_ff: 1536
    dropout: 0.2
    flash_attention: False
  large:
    context_size: 256
    n_layer: 6
    n_head: 8
    d_embed: 512
    d_ff: 2048
    dropout: 0.2
    flash_attention: False
  xl:
    context_size: 256
    n_layer: 8
    n_head: 12
    d_embed: 768
    d_ff: 3072
    dropout: 0.2
    flash_attention: False

# Training
dataset_size:
  xs: 0.01
  small: 0.05
  medium: 0.1
  large: 0.5
  xl: 1.0
epochs: 1  # Fixed
batch_size: 128  # Fixed
optimizer:
  name: AdamW  # AdamW or SGD
  params:
    lr: 0.001
    weight_decay: 0.01
scheduler:
  type: cosine  # linear or cosine
  warmup_ratio: 0.1
grad_clip: 1.0
mixed_precision: False
seed: 101

dataset: shakespeare  # shakespeare or openweb
val_size: 0.1