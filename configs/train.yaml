train_strategy: "steps" # steps or epochs
epochs: 1
max_steps: 1000
val_interval: 500
batch_size: 64
optimizer:
  name: AdamW  # AdamW or SGD
  params:
    lr: 0.0001
    weight_decay: 0.1
scheduler:
  type: cosine  # linear or cosine
  warmup_ratio: 0.1
grad_clip: 1.0
mixed_precision: False
seed: 101

dataset: shakespeare  # shakespeare or openweb
val_size: 0.1

# WandB
project: LLM101-GPT-ContextChar