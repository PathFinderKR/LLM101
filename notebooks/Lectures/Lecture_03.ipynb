{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Lecture 3: Multi-Layer Perceptrons\n",
    "\n",
    "In this lecture, we will introduce Multi-Layer Perceptrons (MLP).\n",
    "\n",
    "We will reproduce the following paper [A Neural Probabilistic Language Model](https://jmlr.org/papers/volume3/bengio03a/bengio03a.pdf)"
   ],
   "id": "284bed26807861ab"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Curse of Dimensionality\n",
    "\n",
    "For a character-level language model, the input vector is a one-hot vector of size 27 (26 characters + 1 space).\n",
    "\n",
    "If the model is not a character-level language model, (e.g., word-level language model), the input vector size is the size of the vocabulary which is usually very large (e.g., 20,000).\n",
    "\n",
    "This leads to the curse of dimensionality.\n",
    "\n",
    "**Solution**: Use a lower-dimensional representation of the input vector.\n",
    "\n",
    "The hypothesis is that similar words will have similar representations (e.g., dog and cat). Let's find a way to embed words into a lower-dimensional space.\n",
    "\n",
    "**Example**\n",
    "- The cat is walking in the bedroom. (Train data)\n",
    "- A dog was running in a room. (Train data)\n",
    "- The cat was running in a room. (Train data)\n",
    "- A dog is walking in a bedroom. (Train data)\n",
    "- A cat was running in a <?> (Test data)\n",
    "\n",
    "The model should be able to predict the word \"room\"(or the similar words) in the test data."
   ],
   "id": "1df5ff9bcc4a9e24"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## MLP\n",
    "\n",
    "In the previous lecture, we have successfully implemented Bigram language model. \n",
    "In this lecture, we will implement a Multi-Layer Perceptron (MLP) language model.\n",
    "\n",
    "For practical reasons, let's use a character-level language model.\n",
    "\n",
    "![MLP](../images/MLP.png)"
   ],
   "id": "3091db96691073c9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Importing Libraries",
   "id": "8708acc13f08c1a2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "b0123d871d2c09a5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-07T10:01:06.267232Z",
     "start_time": "2025-02-07T10:01:04.881648Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from dataclasses import dataclass\n",
    "from src.utils import load_text, set_seed\n",
    "%matplotlib inline"
   ],
   "id": "37a60786dfb334e7",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Configuration",
   "id": "a5ecfeadcb656ced"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-02-07T10:01:06.272116Z",
     "start_time": "2025-02-07T10:01:06.268806Z"
    }
   },
   "source": [
    "@dataclass\n",
    "class MLPConfig:\n",
    "    root_dir: str = os.getcwd() + \"/../../\"\n",
    "    dataset_path: str = \"data/raw/names.txt\"\n",
    "\n",
    "    # Tokenizer\n",
    "    vocab_size: int = 0  # Set later\n",
    "    \n",
    "    # Model\n",
    "    block_size: int = 3\n",
    "    hidden_size: int = 64\n",
    "\n",
    "    seed: int = 101"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Reproducibility",
   "id": "5e753e3f451b6b46"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-07T10:01:06.296089Z",
     "start_time": "2025-02-07T10:01:06.273059Z"
    }
   },
   "cell_type": "code",
   "source": "set_seed(MLPConfig.seed)",
   "id": "95b8b6a2b1a7d33b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed set to 101\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Dataset",
   "id": "d949a0c1c1439c98"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-07T10:01:06.306292Z",
     "start_time": "2025-02-07T10:01:06.297143Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load text and split by lines\n",
    "names = load_text(MLPConfig.root_dir + MLPConfig.dataset_path).splitlines()"
   ],
   "id": "631634a0a4e5f9dc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded text data from /mnt/c/Users/cheir/GitHub/LLM101/notebooks/Lectures/../../data/raw/names.txt (length: 228145 characters).\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Tokenizer",
   "id": "d4e7c1b4ba601a75"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-07T10:01:06.310563Z",
     "start_time": "2025-02-07T10:01:06.307849Z"
    }
   },
   "cell_type": "code",
   "source": [
    "chars = [chr(i) for i in range(97, 123)]  # all alphabet characters\n",
    "chars.insert(0, \".\")  # Add special token\n",
    "MLPConfig.vocab_size = len(chars)\n",
    "str2idx = {char: idx for idx, char in enumerate(chars)}\n",
    "idx2str = {idx: char for char, idx in str2idx.items()}"
   ],
   "id": "dd74c6d04eef45f4",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Preprocessing\n",
    "\n",
    "We need to create a dataset of (Input, Target) pairs.\n",
    "- Input: current 3 characters\n",
    "- Output: next 1 character"
   ],
   "id": "3963012859d3b3ad"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-07T10:07:58.990994Z",
     "start_time": "2025-02-07T10:07:58.987831Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_dataloader(names):\n",
    "    block_size = MLPConfig.block_size  # How many characters do we take to predict the next character\n",
    "    xs, ys = [], []\n",
    "\n",
    "    for name in names:\n",
    "        #print(name)\n",
    "        context = [0] * block_size\n",
    "        \n",
    "        for char in name + \".\":\n",
    "            idx = str2idx[char]\n",
    "            xs.append(context)\n",
    "            ys.append(idx)\n",
    "            #print(''.join(idx2str[i] for i in context), '--->', idx2str[idx])\n",
    "            context = context[1:] + [idx]  # Shift the context by 1 character\n",
    "        \n",
    "        #print(\"=\"*10)\n",
    "\n",
    "    xs = torch.tensor(xs)\n",
    "    ys = torch.tensor(ys)\n",
    "    \n",
    "    return xs, ys"
   ],
   "id": "6cfe842b0b31e4b6",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-07T10:07:59.472040Z",
     "start_time": "2025-02-07T10:07:59.469166Z"
    }
   },
   "cell_type": "code",
   "source": "xs, ys = get_dataloader(names[:3])",
   "id": "48a2ba95dca9476c",
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-07T10:08:13.409546Z",
     "start_time": "2025-02-07T10:08:13.043137Z"
    }
   },
   "cell_type": "code",
   "source": [
    "xs, ys = get_dataloader(names)\n",
    "\n",
    "print(f\"Input shape: {xs.shape}, Output shape: {ys.shape}\")"
   ],
   "id": "8e016a8a62095eb5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.int64 torch.int64\n",
      "Input shape: torch.Size([228146, 3]), Output shape: torch.Size([228146])\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Model",
   "id": "7262c9fc27cdc08a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-07T10:11:20.964403Z",
     "start_time": "2025-02-07T10:11:20.961672Z"
    }
   },
   "cell_type": "code",
   "source": "C = torch.randn(MLPConfig.vocab_size, 2)",
   "id": "85c27cc98404dd9f",
   "outputs": [],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-07T10:11:21.831487Z",
     "start_time": "2025-02-07T10:11:21.828277Z"
    }
   },
   "cell_type": "code",
   "source": "C.shape",
   "id": "7a0a714157c115a7",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([27, 2])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-07T10:16:32.785008Z",
     "start_time": "2025-02-07T10:16:32.781420Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Embedding example\n",
    "# a: [1, :]\n",
    "a_embed = C[1, :]\n",
    "print(f\"Embedding of 'a': {a_embed}\")"
   ],
   "id": "17c51b98f3982eac",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding of 'a': tensor([-1.4056, -0.1122])\n"
     ]
    }
   ],
   "execution_count": 40
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "y = tanh(Cx)\n",
    "- x: (batch_size, block_size)\n",
    "- C: (vocab_size, embedding_size)\n",
    "- "
   ],
   "id": "7e4cda856eb89dc8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "fadace327323382"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "e836179adeea5de1"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
