{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Lecture 6: Batch Normalization and Residual Streams\n",
    "\n",
    "In this lecture, we will discuss two important techniques that have been shown to be very effective in training deep neural networks: Batch Normalization and Residual Streams. We will discuss both of these techniques in detail and show how they can be used to improve the performance of deep neural networks."
   ],
   "id": "d7ec01368ced1292"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Importing libraries",
   "id": "33cf13e36926b778"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T01:52:36.407764Z",
     "start_time": "2025-02-11T01:52:34.068250Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "from dataclasses import dataclass\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn import functional as F\n",
    "from src.utils import load_text, set_seed, configure_device"
   ],
   "id": "45c8dcd7ab524a1d",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Configuration",
   "id": "feba95f7844dc78e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T01:52:36.417091Z",
     "start_time": "2025-02-11T01:52:36.411412Z"
    }
   },
   "cell_type": "code",
   "source": [
    "@dataclass\n",
    "class MLPConfig:\n",
    "    root_dir: str = os.getcwd() + \"/../../\"\n",
    "    dataset_path: str = \"data/names.txt\"\n",
    "    device: torch.device = torch.device('cpu')  # Automatic device configuration\n",
    "\n",
    "    # Tokenizer\n",
    "    vocab_size: int = 0  # Set later\n",
    "\n",
    "    # Model\n",
    "    context_size: int = 3\n",
    "    d_embed: int = 16\n",
    "    d_hidden: int = 256\n",
    "\n",
    "    # Training\n",
    "    val_size: float = 0.1\n",
    "    batch_size: int = 32\n",
    "    max_steps: int = 1000\n",
    "    lr: float = 0.01\n",
    "    val_interval: int = 100\n",
    "    log_interval: int = 100\n",
    "\n",
    "    seed: int = 101"
   ],
   "id": "adb0e0b17be6b9fa",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Reproducibility",
   "id": "ed9be1fb9b578771"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T01:52:38.647425Z",
     "start_time": "2025-02-11T01:52:38.636671Z"
    }
   },
   "cell_type": "code",
   "source": "set_seed(MLPConfig.seed)",
   "id": "df490ff9dffe80b3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed set to 101\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Device",
   "id": "f0f6f62858f630d3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T01:52:38.702689Z",
     "start_time": "2025-02-11T01:52:38.681520Z"
    }
   },
   "cell_type": "code",
   "source": "MLPConfig.device = configure_device()",
   "id": "7ab7090bb2abb213",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on mps\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Dataset",
   "id": "7c8e598d858e5392"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T01:52:38.719790Z",
     "start_time": "2025-02-11T01:52:38.711835Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load text and split by lines\n",
    "names = load_text(MLPConfig.root_dir + MLPConfig.dataset_path).splitlines()"
   ],
   "id": "b61185499088c739",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded text data from /Users/pathfinder/Documents/GitHub/LLM101/notebooks/Lectures/../../data/names.txt (length: 228145 characters).\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Tokenizer",
   "id": "70c9bdafa21b93c2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T01:52:38.750938Z",
     "start_time": "2025-02-11T01:52:38.743862Z"
    }
   },
   "cell_type": "code",
   "source": [
    "chars = [chr(i) for i in range(97, 123)]  # all alphabet characters\n",
    "chars.insert(0, \".\")  # Add special token\n",
    "MLPConfig.vocab_size = len(chars)\n",
    "str2idx = {char: idx for idx, char in enumerate(chars)}\n",
    "idx2str = {idx: char for char, idx in str2idx.items()}"
   ],
   "id": "9a542bb6149dd3c8",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Preprocessing",
   "id": "7798f6842b6f1db8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T01:52:38.786860Z",
     "start_time": "2025-02-11T01:52:38.772361Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Train-Val Split\n",
    "train_names, val_names = train_test_split(names, test_size=MLPConfig.val_size, random_state=MLPConfig.seed)"
   ],
   "id": "8fce6b9d5b6fedd7",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T01:52:39.150775Z",
     "start_time": "2025-02-11T01:52:38.824634Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class NamesDataset(Dataset):\n",
    "    def __init__(self, _names, context_size):\n",
    "        self.inputs, self.targets = [], []\n",
    "\n",
    "        for name in _names:\n",
    "            context = [0] * context_size\n",
    "\n",
    "            for char in name + \".\":\n",
    "                idx = str2idx[char]\n",
    "                self.inputs.append(context)\n",
    "                self.targets.append(idx)\n",
    "                context = context[1:] + [idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_ids = torch.tensor(self.inputs[idx])\n",
    "        target_id = torch.tensor(self.targets[idx])\n",
    "        return input_ids, target_id\n",
    "\n",
    "train_dataset = NamesDataset(train_names, MLPConfig.context_size)\n",
    "val_dataset = NamesDataset(val_names, MLPConfig.context_size)\n",
    "train_loader = DataLoader(train_dataset, batch_size=MLPConfig.batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=MLPConfig.batch_size, shuffle=False)"
   ],
   "id": "bf628da792f33360",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Model",
   "id": "15a30d2d47d50894"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T01:52:39.163225Z",
     "start_time": "2025-02-11T01:52:39.157042Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, vocab_size, context_size, d_embed, d_hidden):\n",
    "        super().__init__()\n",
    "        self.C = nn.Parameter(torch.randn(vocab_size, d_embed))\n",
    "        self.W1 = nn.Parameter(torch.randn(context_size * d_embed, d_hidden))\n",
    "        self.b1 = nn.Parameter(torch.randn(d_hidden))\n",
    "        self.W2 = nn.Parameter(torch.randn(d_hidden, vocab_size))\n",
    "        self.b2 = nn.Parameter(torch.randn(vocab_size))\n",
    "\n",
    "    def forward(self, x):  # x: (batch_size, context_size)\n",
    "        # Embedding\n",
    "        x_embed = self.C[x]  # (batch_size, context_size, d_embed)\n",
    "        x = x_embed.view(x.size(0), -1)  # (batch_size, context_size * d_embed)\n",
    "\n",
    "        # Hidden layer\n",
    "        h = F.tanh(x @ self.W1 + self.b1)  # (batch_size, d_hidden)\n",
    "\n",
    "        # Output layer\n",
    "        logits = torch.matmul(h, self.W2) + self.b2  # (batch_size, vocab_size)\n",
    "        return logits"
   ],
   "id": "5234c969fe1a7d14",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T01:52:39.190695Z",
     "start_time": "2025-02-11T01:52:39.172819Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initialize the model\n",
    "mlp = MLP(MLPConfig.vocab_size, MLPConfig.context_size, MLPConfig.d_embed, MLPConfig.d_hidden)\n",
    "mlp.to(MLPConfig.device) # Move the model to the device\n",
    "print(mlp)\n",
    "print(\"Number of parameters:\", sum(p.numel() for p in mlp.parameters()))"
   ],
   "id": "2f7fba05cab3965d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP()\n",
      "Number of parameters: 19915\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T01:52:39.204602Z",
     "start_time": "2025-02-11T01:52:39.197882Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Training\n",
    "def train(\n",
    "        model: nn.Module,\n",
    "        train_loader: DataLoader,\n",
    "        val_loader: DataLoader,\n",
    "        max_steps: int,\n",
    "        lr: float,\n",
    "        val_interval: int,\n",
    "        log_interval: int,\n",
    "        device: torch.device\n",
    "):\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "    steps = []\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_iter = itertools.cycle(train_loader)  # Infinite dataloader\n",
    "\n",
    "    for step in range(1, max_steps + 1):\n",
    "        model.train()\n",
    "        train_inputs, train_targets = next(train_iter)\n",
    "        train_inputs, train_targets = train_inputs.to(device), train_targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(train_inputs)\n",
    "        loss = F.cross_entropy(logits, train_targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if step % val_interval == 0:\n",
    "            model.eval()\n",
    "            val_loss = 0.0\n",
    "            total_samples = 0\n",
    "            with torch.no_grad():\n",
    "                for val_inputs, val_targets in val_loader:\n",
    "                    val_inputs, val_targets = val_inputs.to(device), val_targets.to(device)\n",
    "                    val_logits = model(val_inputs)\n",
    "                    batch_loss = F.cross_entropy(val_logits, val_targets)\n",
    "                    val_loss += batch_loss.item() * val_inputs.size(0)\n",
    "                    total_samples += val_inputs.size(0)\n",
    "            val_loss /= total_samples\n",
    "            val_losses.append(val_loss)\n",
    "\n",
    "        if step % log_interval == 0:\n",
    "            steps.append(step)\n",
    "            train_losses.append(loss.item())\n",
    "            if step == 1:\n",
    "                print(f\"Initial Train Loss: {loss.item():.4f}\")\n",
    "            print(f\"Step {step}/{max_steps} | Train Loss: {running_loss / step:.4f} | Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "        step += 1\n",
    "\n",
    "    final_val_logits = model(val_inputs.to(device))\n",
    "    final_val_loss = F.cross_entropy(final_val_logits, val_targets.to(device)).item()\n",
    "    print(f\"Final Validation Loss: {final_val_loss:.4f}\")\n",
    "    plt.figure()\n",
    "    plt.plot(steps, train_losses, label=\"Train\")\n",
    "    plt.plot(steps[::val_interval], val_losses, label=\"Validation\")\n",
    "    plt.xlabel(\"Steps\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ],
   "id": "87a94f37ab1cd097",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T01:52:57.863588Z",
     "start_time": "2025-02-11T01:52:39.220410Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train(\n",
    "    model=mlp,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    max_steps=MLPConfig.max_steps,\n",
    "    lr=MLPConfig.lr,\n",
    "    val_interval=MLPConfig.val_interval,\n",
    "    log_interval=MLPConfig.log_interval,\n",
    "    device=MLPConfig.device\n",
    ")"
   ],
   "id": "fa9883141d59837f",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1000/1000 [00:15<00:00, 65.26it/s, loss=13.9290]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Train Loss</td><td>█▆▅▄▃▃▂▂▁▁</td></tr><tr><td>Val Loss</td><td>█▆▄▃▃▂▂▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Train Loss</td><td>13.92902</td></tr><tr><td>Val Loss</td><td>9.56011</td></tr></table><br/></div></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">glowing-salad-4</strong> at: <a href='https://wandb.ai/pathfinderkr/lecture-06/runs/tzhosc4z' target=\"_blank\">https://wandb.ai/pathfinderkr/lecture-06/runs/tzhosc4z</a><br> View project at: <a href='https://wandb.ai/pathfinderkr/lecture-06' target=\"_blank\">https://wandb.ai/pathfinderkr/lecture-06</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Find logs at: <code>./../../wandb/run-20250211_105237-tzhosc4z/logs</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Part 1: Batch Normalization\n",
    "\n",
    "Recall what we do for initializing the weights of a neural network.\n",
    "\n",
    "1. We don't want the logits to be too big because the softmax might explode.\n",
    "- Initialize the final layer with small values.\n",
    "2. We don't want the gradients to vanish.\n",
    "- Initialize the inner layer with small values.\n",
    "- Use different activation functions.\n",
    "\n",
    "Eventually, what we want is to preserve the same gaussian distribution of the activations.\n",
    "\n",
    "**Why not just normalize the activations?** -> Key idea to [Batch Normalization](https://arxiv.org/pdf/1502.03167)"
   ],
   "id": "c4c0f23f26c27931"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T01:52:57.879362Z",
     "start_time": "2025-02-11T01:52:57.873914Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# MLP Model with Batch Normalization\n",
    "class MLPv2(nn.Module):\n",
    "    def __init__(self, vocab_size, context_size, d_embed, d_hidden):\n",
    "        super().__init__()\n",
    "        self.C = nn.Parameter(torch.randn(vocab_size, d_embed))\n",
    "        self.W1 = nn.Parameter(torch.randn(context_size * d_embed, d_hidden))\n",
    "        self.b1 = nn.Parameter(torch.randn(d_hidden))\n",
    "        self.W2 = nn.Parameter(torch.randn(d_hidden, vocab_size))\n",
    "        self.b2 = nn.Parameter(torch.randn(vocab_size))\n",
    "\n",
    "        # Batch Normalization\n",
    "        self.gamma = nn.Parameter(torch.ones(1, d_hidden))\n",
    "        self.beta = nn.Parameter(torch.zeros(1, d_hidden))\n",
    "\n",
    "    def forward(self, x):  # x: (batch_size, context_size)\n",
    "        # Embedding\n",
    "        x_embed = self.C[x]  # (batch_size, context_size, d_embed)\n",
    "        x = x_embed.view(x.size(0), -1)  # (batch_size, context_size * d_embed)\n",
    "\n",
    "        # Hidden layer\n",
    "        x = x @ self.W1 + self.b1  # (batch_size, d_hidden)\n",
    "\n",
    "        # Batch Normalization\n",
    "        ################################################################################\n",
    "        # TODO:                                                                        #\n",
    "        # Implement the MLP model.                                                     #\n",
    "        ################################################################################\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        mean = x.mean(dim=0, keepdim=True)  # (1, d_hidden)\n",
    "        std = x.std(dim=0, keepdim=True)  # (1, d_hidden)\n",
    "        #x = self.gamma * (x - mean) / (std + 1e-6) + self.beta  # (batch_size, d_hidden)\n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "        h = F.tanh(x)  # (batch_size, d_hidden)\n",
    "\n",
    "        # Output layer\n",
    "        logits = torch.matmul(h, self.W2) + self.b2  # (batch_size, vocab_size)\n",
    "        return logits"
   ],
   "id": "a6e12796d6b3e273",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T01:52:57.897192Z",
     "start_time": "2025-02-11T01:52:57.886507Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initialize the model\n",
    "mlpV2 = MLPv2(MLPConfig.vocab_size, MLPConfig.context_size, MLPConfig.d_embed, MLPConfig.d_hidden)\n",
    "mlpV2.to(MLPConfig.device) # Move the model to the device\n",
    "print(mlpV2)\n",
    "print(\"Number of parameters:\", sum(p.numel() for p in mlpV2.parameters()))"
   ],
   "id": "ae2860ff360dde6f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLPv2()\n",
      "Number of parameters: 20427\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T01:52:58.016100Z",
     "start_time": "2025-02-11T01:52:57.903409Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train(\n",
    "    model=mlpV2,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    max_steps=MLPConfig.max_steps,\n",
    "    lr=MLPConfig.lr,\n",
    "    val_interval=MLPConfig.val_interval,\n",
    "    log_interval=MLPConfig.log_interval,\n",
    "    device=MLPConfig.device\n",
    ")"
   ],
   "id": "2fc0182ece5c5fc6",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "Error",
     "evalue": "You must call wandb.init() before wandb.watch()",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mError\u001B[0m                                     Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[16], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m      2\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmlpV2\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      3\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      4\u001B[0m \u001B[43m    \u001B[49m\u001B[43mval_loader\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mval_loader\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      5\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmax_steps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mMLPConfig\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmax_steps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      6\u001B[0m \u001B[43m    \u001B[49m\u001B[43mlr\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mMLPConfig\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlr\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      7\u001B[0m \u001B[43m    \u001B[49m\u001B[43mval_interval\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mMLPConfig\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mval_interval\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      8\u001B[0m \u001B[43m    \u001B[49m\u001B[43mlog_interval\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mMLPConfig\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlog_interval\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      9\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdevice\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mMLPConfig\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdevice\u001B[49m\n\u001B[1;32m     10\u001B[0m \u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[12], line 19\u001B[0m, in \u001B[0;36mtrain\u001B[0;34m(model, train_loader, val_loader, max_steps, lr, val_interval, log_interval, device)\u001B[0m\n\u001B[1;32m     16\u001B[0m train_iter \u001B[38;5;241m=\u001B[39m itertools\u001B[38;5;241m.\u001B[39mcycle(train_loader)  \u001B[38;5;66;03m# Infinite dataloader\u001B[39;00m\n\u001B[1;32m     17\u001B[0m progress_bar \u001B[38;5;241m=\u001B[39m tqdm(total\u001B[38;5;241m=\u001B[39mmax_steps, desc\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTraining\u001B[39m\u001B[38;5;124m\"\u001B[39m, leave\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m---> 19\u001B[0m \u001B[43mwandb\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwatch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlog\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mall\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlog_freq\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlog_interval\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     21\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m step \u001B[38;5;241m<\u001B[39m\u001B[38;5;241m=\u001B[39m max_steps:\n\u001B[1;32m     22\u001B[0m     model\u001B[38;5;241m.\u001B[39mtrain()\n",
      "File \u001B[0;32m/opt/anaconda3/envs/LLM101n/lib/python3.12/site-packages/wandb/sdk/lib/preinit.py:36\u001B[0m, in \u001B[0;36mPreInitCallable.<locals>.preinit_wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     35\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mpreinit_wrapper\u001B[39m(\u001B[38;5;241m*\u001B[39margs: Any, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Any:\n\u001B[0;32m---> 36\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m wandb\u001B[38;5;241m.\u001B[39mError(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mYou must call wandb.init() before \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mname\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m()\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[0;31mError\u001B[0m: You must call wandb.init() before wandb.watch()"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "ASDF",
   "id": "dc06b4c541e6814d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# MLP Model with more layers\n",
    "class MLPv3(nn.Module):\n",
    "    def __init__(self, vocab_size, context_size, d_embed, d_hidden, batch_norm=False):\n",
    "        super().__init__()\n",
    "        self.C = nn.Parameter(torch.randn(vocab_size, d_embed))\n",
    "        self.W1 = nn.Parameter(torch.randn(context_size * d_embed, d_hidden))\n",
    "        self.b1 = nn.Parameter(torch.randn(d_hidden))\n",
    "        self.W2 = nn.Parameter(torch.randn(d_hidden, d_hidden))\n",
    "        self.b2 = nn.Parameter(torch.randn(d_hidden))\n",
    "        self.W3 = nn.Parameter(torch.randn(d_hidden, vocab_size))\n",
    "        self.b3 = nn.Parameter(torch.randn(vocab_size))\n",
    "        self.W4 = nn.Parameter(torch.randn(vocab_size, vocab_size))\n",
    "        self.b4 = nn.Parameter(torch.randn(vocab_size))\n",
    "\n",
    "        # Batch Normalization\n",
    "        self.batch_norm = batch_norm\n",
    "        self.gamma1 = nn.Parameter(torch.ones(1, d_hidden))\n",
    "        self.beta1 = nn.Parameter(torch.zeros(1, d_hidden))\n",
    "        self.gamma2 = nn.Parameter(torch.ones(1, d_hidden))\n",
    "        self.beta2 = nn.Parameter(torch.zeros(1, d_hidden))\n",
    "\n",
    "    def forward(self, x):  # x: (batch_size, context_size)\n",
    "        # Embedding\n",
    "        x_embed = self.C[x]  # (batch_size, context_size, d_embed)\n",
    "        x = x_embed.view(x.size(0), -1)  # (batch_size, context_size * d_embed)\n",
    "\n",
    "        # Hidden layer 1\n",
    "        x = x @ self.W1 + self.b1  # (batch_size, d_hidden)\n",
    "\n",
    "        # Batch Normalization 1\n",
    "        ################################################################################"
   ],
   "id": "c868d6fdbc70eee9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Inference",
   "id": "7cfa0879f4c42b0e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-11T01:52:58.026677Z",
     "start_time": "2025-02-11T01:48:54.136259Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "532ac2ddf62f5d07",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
