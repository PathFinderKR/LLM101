{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Lecture 7: Attention Mechanisms\n",
    "\n",
    "In this lecture, we will introduce attention mechanisms. Attention is the core module in the transformer model, which is the state-of-the-art model for many NLP tasks. Let's reproduce the attention mechanism from scratch step by step."
   ],
   "id": "d70a6266de3d9a75"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Importing libraries",
   "id": "eba822f51503a573"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-02-12T10:13:43.812503Z",
     "start_time": "2025-02-12T10:13:41.493687Z"
    }
   },
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "from dataclasses import dataclass\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn import functional as F\n",
    "from src.utils import load_text, set_seed, configure_device"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Configuration",
   "id": "6d9c4037c51cdb88"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-12T10:13:43.820117Z",
     "start_time": "2025-02-12T10:13:43.816563Z"
    }
   },
   "cell_type": "code",
   "source": [
    "@dataclass\n",
    "class GPTConfig:\n",
    "    root_dir: str = os.getcwd() + \"/../../\"\n",
    "    dataset_path: str = \"data/names.txt\"\n",
    "    device: torch.device = torch.device('cpu')  # Automatic device configuration\n",
    "\n",
    "    # Tokenizer\n",
    "    vocab_size: int = 0  # Set later\n",
    "\n",
    "    # Model\n",
    "    context_size: int = 12\n",
    "    n_layer: int = 3\n",
    "    n_head: int = 4\n",
    "    d_embed: int = 64\n",
    "    d_ff: int = 256\n",
    "    dropout: float = 0.1\n",
    "\n",
    "    # Training\n",
    "    val_size: float = 0.1\n",
    "    batch_size: int = 32\n",
    "    max_steps: int = 1000\n",
    "    lr: float = 0.01\n",
    "    val_interval: int = 100\n",
    "\n",
    "    seed: int = 101\n",
    "\n",
    "config = GPTConfig()"
   ],
   "id": "be57ba8fc9aedcf6",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Reproducibility",
   "id": "37c47b3c933f39c8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-12T10:13:43.914132Z",
     "start_time": "2025-02-12T10:13:43.908978Z"
    }
   },
   "cell_type": "code",
   "source": "set_seed(config.seed)",
   "id": "7a599d81fa45fd03",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed set to 101\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Device",
   "id": "1d64c65aa40f2745"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-12T10:13:43.939524Z",
     "start_time": "2025-02-12T10:13:43.919755Z"
    }
   },
   "cell_type": "code",
   "source": "config.device = configure_device()",
   "id": "6263897fa41a0d5c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on mps\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Dataset",
   "id": "1c5701db44321787"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-12T10:13:43.951822Z",
     "start_time": "2025-02-12T10:13:43.947339Z"
    }
   },
   "cell_type": "code",
   "source": "names = load_text(config.root_dir + config.dataset_path).splitlines()",
   "id": "d733f8aed190ecca",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded text data from /Users/pathfinder/Documents/GitHub/LLM101/notebooks/Lectures/../../data/names.txt (length: 228145 characters).\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Tokenizer",
   "id": "395612ec5147789b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-12T10:13:44.034231Z",
     "start_time": "2025-02-12T10:13:44.028753Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Tokenizer:\n",
    "    def __init__(self, names):\n",
    "        self.names = names\n",
    "        self.vocab = list(set(\"\".join(self.names)))\n",
    "        self.vocab.insert(0, \".\")\n",
    "        self.vocab_size = len(self.vocab)\n",
    "        self.char2idx = {char: idx for idx, char in enumerate(self.vocab)}\n",
    "        self.idx2char = {idx: char for idx, char in enumerate(self.vocab)}\n",
    "\n",
    "    def encode(self, name):\n",
    "        return [self.char2idx[char] for char in name]\n",
    "\n",
    "    def decode(self, tokens):\n",
    "        return \"\".join([self.idx2char[token] for token in tokens])\n",
    "\n",
    "tokenizer = Tokenizer(names)\n",
    "config.vocab_size = tokenizer.vocab_size"
   ],
   "id": "9cc83726df2fc865",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Preprocessing",
   "id": "469ea33a955e63a3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-12T10:13:44.056155Z",
     "start_time": "2025-02-12T10:13:44.048466Z"
    }
   },
   "cell_type": "code",
   "source": "train_names, val_names = train_test_split(names, test_size=config.val_size, random_state=config.seed)",
   "id": "2419b266bf21b884",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class NamesDataset(Dataset):\n",
    "    def __init__(self, _names, context_size):\n",
    "        self.inputs, self.targets = [], []\n",
    "\n",
    "        for name in _names:\n",
    "            context = [0] * context_size\n",
    "\n",
    "            for char in name + \".\":\n",
    "                idx = str2idx[char]\n",
    "                self.inputs.append(context)\n",
    "                self.targets.append(idx)\n",
    "                context = context[1:] + [idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_ids = torch.tensor(self.inputs[idx])\n",
    "        target_id = torch.tensor(self.targets[idx])\n",
    "        return input_ids, target_id\n",
    "\n",
    "train_dataset = NamesDataset(train_names, MLPConfig.context_size)\n",
    "val_dataset = NamesDataset(val_names, MLPConfig.context_size)\n",
    "train_loader = DataLoader(train_dataset, batch_size=MLPConfig.batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=MLPConfig.batch_size, shuffle=False)"
   ],
   "id": "a0cb58bef3e545e7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-12T10:13:44.108088Z",
     "start_time": "2025-02-12T10:13:44.066828Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class NamesDataset(Dataset):\n",
    "    def __init__(self, _names, context_size):\n",
    "        self.inputs, self.targets = [], []\n",
    "\n",
    "        for name in _names:\n",
    "            tokens = [tokenizer.encode(char) for char in name + \".\"]  # Convert name to token indices\n",
    "\n",
    "            # Create context windows\n",
    "            for i in range(len(tokens) - context_size):  # Ensure enough tokens for full sequence\n",
    "                context = tokens[i:i + context_size]  # Input sequence\n",
    "                target = tokens[i + 1:i + context_size + 1]  # Shifted target sequence\n",
    "                self.inputs.append(context)\n",
    "                self.targets.append(target)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_ids = torch.tensor(self.inputs[idx])  # Shape: (context_size,)\n",
    "        target_ids = torch.tensor(self.targets[idx])  # Shape: (context_size,)\n",
    "        return input_ids, target_ids\n",
    "\n",
    "train_dataset = NamesDataset(train_names, config.context_size)\n",
    "val_dataset = NamesDataset(val_names, config.context_size)\n",
    "train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=config.batch_size, shuffle=False)"
   ],
   "id": "276192d1a547283d",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-12T10:13:44.117948Z",
     "start_time": "2025-02-12T10:13:44.113583Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for inputs, targets in train_loader:\n",
    "    print(inputs.shape, targets.shape)\n",
    "    break"
   ],
   "id": "47494c13930fd878",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 12, 1]) torch.Size([32, 12, 1])\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-12T10:13:44.221515Z",
     "start_time": "2025-02-12T10:13:44.129319Z"
    }
   },
   "cell_type": "code",
   "source": "adf",
   "id": "1334fe809568697f",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'adf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[10], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43madf\u001B[49m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'adf' is not defined"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Model",
   "id": "d70bd7cb9c06609"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Casual Self-Attention",
   "id": "78bf5cbc9a92270"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-12T10:13:44.228423Z",
     "start_time": "2025-02-12T10:12:00.473701Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class CasualSelfAttention(nn.Module):\n",
    "    def __init__(self, d_embed: int, n_head: int, dropout: float):\n",
    "        super(CasualSelfAttention, self).__init__()\n",
    "        self.n_head = n_head\n",
    "        self.d_head = d_embed // n_head\n",
    "        self.scale = self.d_head ** -0.5\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.query = nn.Linear(d_embed, d_embed, bias=False)\n",
    "        self.key = nn.Linear(d_embed, d_embed, bias=False)\n",
    "        self.value = nn.Linear(d_embed, d_embed, bias=False)\n",
    "        self.out = nn.Linear(d_embed, d_embed, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, context_size, _ = x.size()\n",
    "\n",
    "        # Query, Key, Value\n",
    "        q = self.query(x)  # (batch_size, context_size, d_embed)\n",
    "        k = self.key(x)  # (batch_size, context_size, d_embed)\n",
    "        v = self.value(x)  # (batch_size, context_size, d_embed)\n",
    "        q = q.view(batch_size, context_size, self.n_head, self.d_head).transpose(1, 2)  # (batch_size, n_head, context_size, d_head)\n",
    "        k = k.view(batch_size, context_size, self.n_head, self.d_head).transpose(1, 2)  # (batch_size, n_head, context_size, d_head)\n",
    "        v = v.view(batch_size, context_size, self.n_head, self.d_head).transpose(1, 2)  # (batch_size, n_head, context_size, d_head)\n",
    "\n",
    "        # Scaled Dot-Product Attention\n",
    "        attn_scores = torch.matmul(q, k.transpose(-2, -1)) * self.scale  # (batch_size, n_head, context_size, context_size)\n",
    "\n",
    "        # Masking\n",
    "        mask = torch.triu(torch.ones(context_size, context_size, device=x.device), diagonal=1).bool()  # (context_size, context_size)\n",
    "        attn_scores = attn_scores.masked_fill(mask[None, None, :, :], float('-inf'))  # (batch_size, n_head, context_size, context_size)\n",
    "\n",
    "        # Softmax\n",
    "        attn_scores = F.softmax(attn_scores, dim=-1)  # (batch_size, n_head, context_size, context_size)\n",
    "\n",
    "        # Dropout\n",
    "        attn_scores = F.dropout(attn_scores, p=self.dropout, training=self.training)  # (batch_size, n_head, context_size, context_size)\n",
    "\n",
    "        # Weighted Sum\n",
    "        attn_output = torch.matmul(attn_scores, v)  # (batch_size, n_head, context_size, d_head)\n",
    "\n",
    "        # Concatenation\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, context_size, self.n_head * self.d_head)  # (batch_size, context_size, d_embed)\n",
    "\n",
    "        # Output Linear Layer\n",
    "        x = self.out(attn_output)  # (batch_size, context_size, d_embed)\n",
    "        return x\n"
   ],
   "id": "6c99370e0415ee29",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Feed Forward",
   "id": "fe29a70a5cdb64cf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-12T10:13:44.229424Z",
     "start_time": "2025-02-12T10:12:00.482655Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_embed: int, d_ff: int, dropout: float):\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.fc1 = nn.Linear(d_embed, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_embed)\n",
    "        self.activation = nn.GELU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.dropout(x)\n",
    "        return x"
   ],
   "id": "91561c42bb05fa07",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Decoder Layer",
   "id": "cecf0a41442bbc19"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-12T10:13:44.229689Z",
     "start_time": "2025-02-12T10:12:00.489927Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_embed: int, n_head: int, d_ff: int, dropout: float):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.self_attention = CasualSelfAttention(d_embed, n_head, dropout)\n",
    "        self.layer_norm1 = nn.LayerNorm(d_embed)\n",
    "        self.feed_forward = FeedForward(d_embed, d_ff, dropout)\n",
    "        self.layer_norm2 = nn.LayerNorm(d_embed)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.self_attention(self.layer_norm1(x))\n",
    "        x = x + self.feed_forward(self.layer_norm2(x))\n",
    "        return x"
   ],
   "id": "e4ca7c7ef5da3397",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### GPT",
   "id": "b4dd6274ace2d58c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-12T10:13:44.229868Z",
     "start_time": "2025-02-12T10:12:00.497830Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class GPT(nn.Module):\n",
    "    def __init__(self, vocab_size: int, context_size: int, d_embed: int, n_head: int, d_ff: int, n_layer: int, dropout: float):\n",
    "        super(GPT, self).__init__()\n",
    "        self.context_size = context_size\n",
    "        self.token_embedding = nn.Embedding(vocab_size, d_embed)\n",
    "        self.positional_embedding = nn.Embedding(context_size, d_embed)\n",
    "        self.layers = nn.ModuleList([DecoderLayer(d_embed, n_head, d_ff, dropout) for _ in range(n_layer)])\n",
    "        self.layer_norm = nn.LayerNorm(d_embed)\n",
    "        self.linear = nn.Linear(d_embed, vocab_size)\n",
    "\n",
    "    def forward(self, x):  # x: (batch_size, context_size)\n",
    "        batch_size, context_size = x.size()\n",
    "        assert context_size <= self.context_size, \\\n",
    "            f\"context_size should be less than or equal to {self.config.context_size}, but got {context_size}\"\n",
    "\n",
    "        # Embedding\n",
    "        token_embed = self.token_embedding(x)  # (batch_size, context_size, d_embed)\n",
    "        pos_idx = torch.arange(context_size, device=x.device)  # (context_size)\n",
    "        pos_embed = self.positional_embedding(pos_idx)  # (batch_size, context_size, d_embed)\n",
    "        x = token_embed + pos_embed  # (batch_size, context_size, d_embed)\n",
    "\n",
    "        # Decoder layers\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)  # (batch_size, context_size, d_embed)\n",
    "\n",
    "        # Output\n",
    "        x = self.layer_norm(x)\n",
    "        x = self.linear(x)  # (batch_size, context_size, vocab_size)\n",
    "        return x\n",
    "\n",
    "    def loss (self, logits, targets):\n",
    "        logits = logits.view(-1, logits.size(-1))  # (batch_size * context_size, vocab_size)\n",
    "        targets = targets.view(-1)  # (batch_size * context_size)\n",
    "        return F.cross_entropy(logits, targets)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, tokenizer, device):\n",
    "        self.eval()\n",
    "        end_id = tokenizer.encode(\".\")\n",
    "        context = [end_id] * self.context_size\n",
    "        context = torch.tensor(context, dtype=torch.long, device=device).unsqueeze(0)  # Shape: (1, context_size)\n",
    "\n",
    "        # Generation loop\n",
    "        while True:\n",
    "            # Truncate\n",
    "            context = context[:, -self.config.context_size:]  # (batch_size=1, context_size)\n",
    "\n",
    "            # Forward\n",
    "            logits = self.forward(context)[:, -1, :] # (batch_size=1, vocab_size)\n",
    "\n",
    "            # Sample\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)  # (batch_size=1, 1)\n",
    "\n",
    "            # Concatenate\n",
    "            context = torch.cat((context, next_token), dim=-1)  # (batch_size=1, context_size + 1)\n",
    "\n",
    "            # Decode\n",
    "            name_char = tokenizer.decode([next_token[0].item()])\n",
    "            print(name_char, end='', flush=True)\n",
    "\n",
    "            # Break if \".\"\n",
    "            if next_token[0].item() == end_id:\n",
    "                break\n"
   ],
   "id": "252da1075abc399c",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-12T10:13:44.240350Z",
     "start_time": "2025-02-12T10:12:00.509756Z"
    }
   },
   "cell_type": "code",
   "source": [
    "gpt = GPT(\n",
    "    vocab_size=config.vocab_size,\n",
    "    context_size=config.context_size,\n",
    "    d_embed=config.d_embed,\n",
    "    n_head=config.n_head,\n",
    "    d_ff=config.d_ff,\n",
    "    n_layer=config.n_layer,\n",
    "    dropout=config.dropout\n",
    ").to(config.device)\n",
    "print(gpt)\n",
    "print(f\"Number of parameters: {sum(p.numel() for p in gpt.parameters() if p.requires_grad)}\")"
   ],
   "id": "afd4c978bd93f6c2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT(\n",
      "  (token_embedding): Embedding(27, 64)\n",
      "  (positional_embedding): Embedding(12, 64)\n",
      "  (layers): ModuleList(\n",
      "    (0-2): 3 x DecoderLayer(\n",
      "      (self_attention): CasualSelfAttention(\n",
      "        (query): Linear(in_features=64, out_features=64, bias=False)\n",
      "        (key): Linear(in_features=64, out_features=64, bias=False)\n",
      "        (value): Linear(in_features=64, out_features=64, bias=False)\n",
      "        (out): Linear(in_features=64, out_features=64, bias=False)\n",
      "      )\n",
      "      (layer_norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      (feed_forward): FeedForward(\n",
      "        (fc1): Linear(in_features=64, out_features=256, bias=True)\n",
      "        (fc2): Linear(in_features=256, out_features=64, bias=True)\n",
      "        (activation): GELU(approximate='none')\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (layer_norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "  (linear): Linear(in_features=64, out_features=27, bias=True)\n",
      ")\n",
      "Number of parameters: 153563\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Training",
   "id": "1d1d9a614384a0e1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-12T10:13:44.241049Z",
     "start_time": "2025-02-12T10:12:00.544570Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train(\n",
    "        model: nn.Module,\n",
    "        train_loader: DataLoader,\n",
    "        val_loader: DataLoader,\n",
    "        max_steps: int,\n",
    "        lr: float,\n",
    "        val_interval: int,\n",
    "        device: torch.device\n",
    "):\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "    steps = []\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_iter = itertools.cycle(train_loader)  # Infinite dataloader\n",
    "\n",
    "    for step in range(1, max_steps + 1):\n",
    "        model.train()\n",
    "        train_inputs, train_targets = next(train_iter)\n",
    "        train_inputs, train_targets = train_inputs.to(device), train_targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(train_inputs)\n",
    "        loss = model.loss(logits, train_targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        steps.append(step)\n",
    "        train_losses.append(loss.item())\n",
    "\n",
    "        if step % val_interval == 0:\n",
    "            model.eval()\n",
    "            val_loss = 0.0\n",
    "            total_samples = 0\n",
    "            with torch.no_grad():\n",
    "                for val_inputs, val_targets in val_loader:\n",
    "                    val_inputs, val_targets = val_inputs.to(device), val_targets.to(device)\n",
    "                    val_logits = model(val_inputs)\n",
    "                    batch_loss = model.loss(val_logits, val_targets)\n",
    "                    val_loss += batch_loss.item() * val_inputs.size(0)\n",
    "                    total_samples += val_inputs.size(0)\n",
    "            val_loss /= total_samples\n",
    "            val_losses.append(val_loss)\n",
    "            print(f\"Step {step}: Train Loss = {loss.item():.4f}, Val Loss = {val_loss:.4f}\")\n",
    "\n",
    "        if step == 1:\n",
    "            print(f\"Initial Train Loss = {loss.item():.4f}\")\n",
    "\n",
    "    # Plot the loss\n",
    "    plt.figure()\n",
    "    plt.plot(steps, train_losses, label=\"Train\")\n",
    "    val_steps = [step for step in steps if step % val_interval == 0]\n",
    "    plt.plot(val_steps, val_losses, label=\"Validation\")\n",
    "    plt.xlabel(\"Steps\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ],
   "id": "87ed4448d92e66c",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-12T10:13:44.242461Z",
     "start_time": "2025-02-12T10:12:00.560946Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train(\n",
    "    model=gpt,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    max_steps=config.max_steps,\n",
    "    lr=config.lr,\n",
    "    val_interval=config.val_interval,\n",
    "    device=config.device\n",
    ")"
   ],
   "id": "1b08d3b4c9d48c04",
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[15], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m      2\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgpt\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      3\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      4\u001B[0m \u001B[43m    \u001B[49m\u001B[43mval_loader\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mval_loader\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      5\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmax_steps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmax_steps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      6\u001B[0m \u001B[43m    \u001B[49m\u001B[43mlr\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlr\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      7\u001B[0m \u001B[43m    \u001B[49m\u001B[43mval_interval\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mval_interval\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      8\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdevice\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdevice\u001B[49m\n\u001B[1;32m      9\u001B[0m \u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[14], line 21\u001B[0m, in \u001B[0;36mtrain\u001B[0;34m(model, train_loader, val_loader, max_steps, lr, val_interval, device)\u001B[0m\n\u001B[1;32m     19\u001B[0m train_inputs, train_targets \u001B[38;5;241m=\u001B[39m train_inputs\u001B[38;5;241m.\u001B[39mto(device), train_targets\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[1;32m     20\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[0;32m---> 21\u001B[0m logits \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_inputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     22\u001B[0m loss \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mloss(logits, train_targets)\n\u001B[1;32m     23\u001B[0m loss\u001B[38;5;241m.\u001B[39mbackward()\n",
      "File \u001B[0;32m/opt/anaconda3/envs/LLM101n/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1737\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1738\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1739\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/anaconda3/envs/LLM101n/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1745\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1746\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1747\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1748\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1749\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1750\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1752\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1753\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "Cell \u001B[0;32mIn[12], line 12\u001B[0m, in \u001B[0;36mGPT.forward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m     11\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):  \u001B[38;5;66;03m# x: (batch_size, context_size)\u001B[39;00m\n\u001B[0;32m---> 12\u001B[0m     batch_size, context_size \u001B[38;5;241m=\u001B[39m x\u001B[38;5;241m.\u001B[39msize()\n\u001B[1;32m     13\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m context_size \u001B[38;5;241m<\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcontext_size, \\\n\u001B[1;32m     14\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcontext_size should be less than or equal to \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39mcontext_size\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, but got \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mcontext_size\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     16\u001B[0m     \u001B[38;5;66;03m# Embedding\u001B[39;00m\n",
      "\u001B[0;31mValueError\u001B[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Inference",
   "id": "31f57a38b05be19b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-12T10:13:44.242937Z",
     "start_time": "2025-02-12T09:59:38.993375Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for _ in range(5):\n",
    "    gpt.generate(tokenizer, config.device)\n",
    "    print()"
   ],
   "id": "c78bf4aa4a5bec27",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      "kw.\n"
     ]
    }
   ],
   "execution_count": 17
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
