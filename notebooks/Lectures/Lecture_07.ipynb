{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Lecture 7: Attention Mechanisms\n",
    "\n",
    "In this lecture, we will introduce attention mechanisms. Attention is the core module in the transformer model, which is the state-of-the-art model for many NLP tasks. Let's reproduce the attention mechanism from scratch step by step."
   ],
   "id": "d70a6266de3d9a75"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Importing libraries",
   "id": "eba822f51503a573"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from dataclasses import dataclass\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn import functional as F\n",
    "from src.utils import load_text, set_seed, configure_device"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Configuration",
   "id": "6d9c4037c51cdb88"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "@dataclass\n",
    "class GPTConfig:\n",
    "    root_dir: str = os.getcwd() + \"/../../\"\n",
    "    dataset_path: str = \"data/names.txt\"\n",
    "    device: torch.device = torch.device('cpu')  # Automatic device configuration\n",
    "\n",
    "    # Tokenizer\n",
    "    vocab_size: int = 0  # Set later\n",
    "\n",
    "    # Model\n",
    "    context_size: int = 3\n",
    "\n",
    "\n",
    "    # Training\n",
    "    val_size: float = 0.1\n",
    "    batch_size: int = 32\n",
    "    max_steps: int = 1000\n",
    "    lr: float = 0.01\n",
    "    val_interval: int = 100\n",
    "\n",
    "    seed: int = 101"
   ],
   "id": "be57ba8fc9aedcf6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Reproducibility",
   "id": "37c47b3c933f39c8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "set_seed(GPTConfig.seed)",
   "id": "7a599d81fa45fd03"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Device",
   "id": "1d64c65aa40f2745"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "GPTConfig.device = configure_device()",
   "id": "6263897fa41a0d5c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Dataset",
   "id": "1c5701db44321787"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "names = load_text(GPTConfig.root_dir + GPTConfig.dataset_path).splitlines()",
   "id": "d733f8aed190ecca"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Tokenizer",
   "id": "395612ec5147789b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class Tokenizer:\n",
    "    def __init__(self, names):\n",
    "        self.names = names\n",
    "        self.vocab = list(set(\"\".join(self.names)))\n",
    "        self.vocab_size = len(self.vocab)\n",
    "        self.char2idx = {char: idx for idx, char in enumerate(self.vocab)}\n",
    "        self.idx2char = {idx: char for idx, char in enumerate(self.vocab)}\n",
    "\n",
    "    def encode(self, name):\n",
    "        return [self.char2idx[char] for char in name]\n",
    "\n",
    "    def decode(self, tokens):\n",
    "        return \"\".join([self.idx2char[token] for token in tokens])\n",
    "\n",
    "tokenizer = Tokenizer(names)"
   ],
   "id": "9cc83726df2fc865"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Preprocessing",
   "id": "469ea33a955e63a3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class NameDataset(Dataset):\n",
    "    def __init__(self, names, tokenizer):\n",
    "        self.names = names\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.names)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        name = self.names[idx]\n",
    "        return self.tokenizer.encode(name)\n",
    "\n",
    "train_dataset = NameDataset(names, tokenizer)\n"
   ],
   "id": "276192d1a547283d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Model",
   "id": "d70bd7cb9c06609"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "252da1075abc399c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Attention Mechanism",
   "id": "1dbf57a09ba678a3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "441ea2beecbfc599"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Training",
   "id": "1d1d9a614384a0e1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "87ed4448d92e66c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Inference",
   "id": "31f57a38b05be19b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "20c446377f2fad50"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
