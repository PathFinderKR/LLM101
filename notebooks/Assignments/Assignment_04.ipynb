{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Assignment 4\n",
    "\n",
    "In this assignment,"
   ],
   "id": "bbbb1cc93fe3e1c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Importing Libraries",
   "id": "fe1e201da9092e12"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-10T02:45:27.366268Z",
     "start_time": "2025-02-10T02:45:24.890380Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Tuple\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from src.utils import load_text, set_seed, configure_device\n",
    "%matplotlib inline"
   ],
   "id": "e767845154ac8dff",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Configuration",
   "id": "37475b8f118bf62f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-10T02:45:27.376670Z",
     "start_time": "2025-02-10T02:45:27.372717Z"
    }
   },
   "cell_type": "code",
   "source": [
    "@dataclass\n",
    "class MLPConfig:\n",
    "    root_dir: str = os.getcwd() + \"/../../\"\n",
    "    dataset_path: str = \"data/names.txt\"\n",
    "    device: torch.device = torch.device('cpu')  # Automatic device configuration\n",
    "\n",
    "    # Tokenizer\n",
    "    vocab_size: int = 0  # Set later\n",
    "    \n",
    "    # Model\n",
    "    context_size: int = 3\n",
    "    d_embed: int = 16\n",
    "    d_hidden: int = 64\n",
    "    \n",
    "    # Training\n",
    "    val_size: float = 0.1\n",
    "    val_interval: int = 1000\n",
    "    batch_size: int = 32\n",
    "    lr: float = 2e-3\n",
    "    max_steps: int = 10000\n",
    "\n",
    "    seed: int = 101"
   ],
   "id": "ab74c7e96ee2918c",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Reproducibility",
   "id": "b37b1da5eb884f97"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-10T02:45:27.496279Z",
     "start_time": "2025-02-10T02:45:27.491049Z"
    }
   },
   "cell_type": "code",
   "source": "set_seed(MLPConfig.seed)",
   "id": "5aac9ba3a2c94cdd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed set to 101\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Device",
   "id": "8046aa4cb3a6469f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-10T02:45:27.593983Z",
     "start_time": "2025-02-10T02:45:27.564314Z"
    }
   },
   "cell_type": "code",
   "source": "MLPConfig.device = configure_device()",
   "id": "ec9748db8884490e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on mps\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Tokenizer",
   "id": "db4d6ad274a6fbbb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-10T02:45:27.675632Z",
     "start_time": "2025-02-10T02:45:27.671031Z"
    }
   },
   "cell_type": "code",
   "source": [
    "chars = [chr(i) for i in range(97, 123)]  # all alphabet characters\n",
    "chars.insert(0, \".\")  # Add special token\n",
    "MLPConfig.vocab_size = len(chars)\n",
    "str2idx = {char: idx for idx, char in enumerate(chars)}\n",
    "idx2str = {idx: char for char, idx in str2idx.items()}"
   ],
   "id": "e9dbce085edefdc4",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Dataset",
   "id": "576b370aea4c1555"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-10T02:45:27.696433Z",
     "start_time": "2025-02-10T02:45:27.689792Z"
    }
   },
   "cell_type": "code",
   "source": "names = load_text(MLPConfig.root_dir + MLPConfig.dataset_path).splitlines()",
   "id": "8cc4922d3b541a53",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded text data from /Users/pathfinder/Documents/GitHub/LLM101/notebooks/Assignments/../../data/names.txt (length: 228145 characters).\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Preprocessing",
   "id": "31bf63ac06c3b24c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-10T02:45:27.816425Z",
     "start_time": "2025-02-10T02:45:27.806331Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Train-Val Split\n",
    "train_names, val_names = train_test_split(names, test_size=MLPConfig.val_size, random_state=MLPConfig.seed)"
   ],
   "id": "f91738376b5a8431",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-10T02:45:29.158244Z",
     "start_time": "2025-02-10T02:45:29.155185Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(f\"Train Size: {len(train_names)}\")\n",
    "print(f\"Validation Size: {len(val_names)}\")\n",
    "print(f\"Train Example: {train_names[0]}\")\n",
    "print(f\"Validation Example: {val_names[0]}\")"
   ],
   "id": "9f27ec069c3321f8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Size: 28829\n",
      "Validation Size: 3204\n",
      "Train Example: keyler\n",
      "Validation Example: jessamae\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-10T02:45:31.124942Z",
     "start_time": "2025-02-10T02:45:31.121365Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def prepare_dataset(_names):\n",
    "    _inputs, _targets = [], []\n",
    "\n",
    "    for name in _names:\n",
    "        context = [0] * MLPConfig.context_size\n",
    "\n",
    "        for char in name + \".\":\n",
    "            idx = str2idx[char]\n",
    "            _inputs.append(context)\n",
    "            _targets.append(idx)\n",
    "            context = context[1:] + [idx]  # Shift the context by 1 character\n",
    "\n",
    "    _inputs = torch.tensor(_inputs)\n",
    "    _targets = torch.tensor(_targets)\n",
    "\n",
    "    return _inputs, _targets"
   ],
   "id": "44931589173ddd84",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Task 1: PyTorch DataLoader\n",
    "\n",
    "We have been using plain Python lists to and then converted them to PyTorch tensors. This is not efficient since it is loading the entire dataset into memory.\n",
    "\n",
    "PyTorch provides `Dataset` and `DataLoader` class to load the data in memory on the fly. [PyTorch Documentation](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html)\n",
    "\n",
    "Refactor the `prepare_dataset` function into a PyTorch `Dataset` class and use the `DataLoader` to efficiently load the data in batches."
   ],
   "id": "27772dc3e0015a64"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-10T02:45:50.026080Z",
     "start_time": "2025-02-10T02:45:50.021960Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Dataset\n",
    "class NamesDataset(Dataset):\n",
    "    ################################################################################\n",
    "    # TODO:                                                                        #\n",
    "    # PyTorch Dataset requires 3 methods:                                          #\n",
    "    # __init__ method to initialize the dataset                                    #\n",
    "    # __len__ method to return the size of the dataset                             #\n",
    "    # __getitem__ method to return a sample from the dataset                       #\n",
    "    ################################################################################\n",
    "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    def __init__(self, _names: List[str], context_size: int):\n",
    "        \"\"\"\n",
    "        Initialize the dataset\n",
    "\n",
    "        Args:\n",
    "            _names (List[str]): List of names\n",
    "            context_size (int): Context size of the model\n",
    "        \"\"\"\n",
    "        self.inputs, self.targets = [], []\n",
    "\n",
    "        for name in _names:\n",
    "            context = [0] * context_size\n",
    "\n",
    "            for char in name + \".\":\n",
    "                idx = str2idx[char]\n",
    "                self.inputs.append(context)\n",
    "                self.targets.append(idx)\n",
    "                context = context[1:] + [idx]  # Shift the context by 1 character\n",
    "        \n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"\n",
    "        Return the size of the dataset\n",
    "\n",
    "        Returns:\n",
    "            (int): Size of the dataset\n",
    "        \"\"\"\n",
    "        return len(self.inputs)\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Return a sample from the dataset\n",
    "\n",
    "        Args:\n",
    "            idx (int): Index of the sample\n",
    "\n",
    "        Returns:\n",
    "            Tuple[torch.Tensor, torch.Tensor]: Input and target tensors\n",
    "        \"\"\"\n",
    "        input_ids = torch.tensor(self.inputs[idx])\n",
    "        target_id = torch.tensor(self.targets[idx])\n",
    "        return input_ids, target_id\n",
    "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****"
   ],
   "id": "b550956d3a003a7e",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-10T02:45:50.371861Z",
     "start_time": "2025-02-10T02:45:50.223748Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initialize the dataset\n",
    "train_dataset = NamesDataset(train_names, MLPConfig.context_size)\n",
    "val_dataset = NamesDataset(val_names, MLPConfig.context_size)"
   ],
   "id": "352be875ddc0fa7a",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-10T02:46:50.972526Z",
     "start_time": "2025-02-10T02:46:50.957081Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(f\"Number of Train Samples: {len(train_dataset)}\")\n",
    "print(f\"Number of Validation Samples: {len(val_dataset)}\")\n",
    "print(f\"First train (input, target): {train_dataset[0]}\")\n",
    "print(f\"First validation (input, target): {val_dataset[0]}\")\n",
    "print(f\"Second train (input, target): {train_dataset[1]}\")\n",
    "print(f\"Second validation (input, target): {val_dataset[1]}\")"
   ],
   "id": "27812caaf7fd7ea6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dataset: 205456\n",
      "Validation Dataset: 22690\n",
      "First train (input, target): (tensor([0, 0, 0]), tensor(11))\n",
      "First validation (input, target): (tensor([0, 0, 0]), tensor(10))\n",
      "Second train (input, target): (tensor([ 0,  0, 11]), tensor(5))\n",
      "Second validation (input, target): (tensor([ 0,  0, 10]), tensor(5))\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-10T00:32:43.933040Z",
     "start_time": "2025-02-10T00:32:43.929520Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# DataLoader\n",
    "################################################################################\n",
    "# TODO:                                                                        #\n",
    "# Initialize the DataLoader for the training and validation datasets.          #\n",
    "################################################################################\n",
    "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "train_loader = DataLoader(train_dataset, batch_size=MLPConfig.batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=MLPConfig.batch_size, shuffle=False)\n",
    "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****"
   ],
   "id": "a9e04718940ac55d",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-10T00:33:43.859053Z",
     "start_time": "2025-02-10T00:33:43.841129Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Example batch\n",
    "for train_batch in train_loader:\n",
    "    print(f\"Train Batch: {train_batch}\")\n",
    "    break\n",
    "\n",
    "for val_batch in val_loader:\n",
    "    print(f\"Validation Batch: {val_batch}\")\n",
    "    break"
   ],
   "id": "13763508cb10bf45",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Batch: ['jerico', 'royal', 'sophia', 'braxon', 'melayah', 'jael', 'anela', 'corina', 'rockford', 'makani', 'jacere', 'cooper', 'eudora', 'farrell', 'kristel', 'fischer', 'coley', 'malaia', 'tavaris', 'derez', 'jo', 'berklie', 'colson', 'evvy', 'xaila', 'aynslee', 'leith', 'leighanne', 'senna', 'anura', 'betania', 'rein']\n",
      "Validation Batch: ['jessamae', 'haygen', 'emoree', 'rayvn', 'jemuel', 'evelena', 'asal', 'ayleen', 'lucile', 'shun', 'aayushi', 'derrick', 'antwone', 'hikari', 'donya', 'maeson', 'henrick', 'quinnly', 'reik', 'oswaldo', 'zarayah', 'annemarie', 'brette', 'quantrell', 'sigrun', 'amorina', 'taylie', 'braxdon', 'eyad', 'kloni', 'hudayfa', 'miguelangel']\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Model",
   "id": "77b0001776a8667d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Task 2: MLP Model\n",
    "\n",
    "Initialize the weights of the model using the `Kaiming` initialization."
   ],
   "id": "5bec79a9421554b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class MLP(nn.Module):\n",
    "    ################################################################################\n",
    "    # TODO:                                                                        #\n",
    "    # Define the __init__ and forward methods for the MLP model.                   #\n",
    "    ################################################################################\n",
    "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    def __init__(self, vocab_size, context_size, d_embed, d_hidden):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_embed)\n",
    "        self.linear1 = nn.Linear(context_size * d_embed, d_hidden, bias=True)\n",
    "        self.linear2 = nn.Linear(d_hidden, vocab_size, bias=True)\n",
    "        \n",
    "    def forward(self, x):  # x: (batch_size, context_size)\n",
    "        x_embed = self.embedding(x)  # (batch_size, context_size, d_embed)\n",
    "        x_embed = x_embed.view(x_embed.size(0), -1)  # (batch_size, context_size * d_embed)\n",
    "        x = F.relu(self.linear1(x_embed))  # (batch_size, d_hidden)\n",
    "        x = self.linear2(x)  # (batch_size, vocab_size)\n",
    "        return x\n",
    "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****"
   ],
   "id": "e5ce5b250ec839a3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Initialize the model\n",
    "mlp = MLP(MLPConfig.vocab_size, MLPConfig.context_size, MLPConfig.d_embed, MLPConfig.d_hidden)\n",
    "mlp.to(MLPConfig.device) # Move the model to the device\n",
    "print(mlp)\n",
    "print(\"Number of parameters:\", sum(p.numel() for p in mlp.parameters()))"
   ],
   "id": "e70046f646dc9e37"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Training",
   "id": "62d6bd917a11788c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Task 3: Training loop",
   "id": "25a58f16a9e13134"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def train(model, max_steps=MLPConfig.max_steps):\n",
    "    steps = []\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    # Define the optimizer\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=MLPConfig.lr)\n",
    "    \n",
    "    for step in range(1, max_steps + 1):\n",
    "        # Training\n",
    "        # Sample batch\n",
    "        idx = torch.randperm(len(train_inputs))[:MLPConfig.batch_size]\n",
    "        x, y = train_inputs[idx], train_targets[idx]\n",
    "        x, y = x.to(MLPConfig.device), y.to(MLPConfig.device)  # Move the data to the device\n",
    "        \n",
    "        ################################################################################\n",
    "        # TODO:                                                                        #\n",
    "        # Implement the forward pass and the backward pass                             #\n",
    "        ################################################################################\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(x)\n",
    "        # print(logits[0])\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        \n",
    "        # Validation\n",
    "        if step % MLPConfig.val_interval == 0:\n",
    "            # Validation loss\n",
    "            with torch.no_grad():\n",
    "                val_logits = model(val_inputs.to(MLPConfig.device))\n",
    "                val_loss = F.cross_entropy(val_logits, val_targets.to(MLPConfig.device)).item()\n",
    "                val_losses.append(val_loss)\n",
    "            \n",
    "        # Logging\n",
    "        steps.append(step)\n",
    "        train_losses.append(loss.item())\n",
    "        if step == 1:\n",
    "            print(f\"Initial Train Loss = {loss.item():.4f}\")\n",
    "        if step % MLPConfig.val_interval == 0:\n",
    "            print(f\"Step {step}: Train Loss = {loss.item():.4f}, Val Loss = {val_loss:.4f}\")\n",
    "    \n",
    "    final_val_logits = model(val_inputs.to(MLPConfig.device))\n",
    "    final_val_loss = F.cross_entropy(final_val_logits, val_targets.to(MLPConfig.device)).item()\n",
    "    print(f\"Final Validation Loss = {final_val_loss:.4f}\")\n",
    "\n",
    "    # Plot the loss\n",
    "    if max_steps > MLPConfig.val_interval:\n",
    "        plt.figure()\n",
    "        plt.plot(steps, train_losses, label=\"Train\")\n",
    "        plt.plot(steps[::MLPConfig.val_interval], val_losses, label=\"Validation\")\n",
    "        plt.xlabel(\"Steps\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.legend()\n",
    "        plt.show()"
   ],
   "id": "14d0ce97e686761b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "train(model=mlp, max_steps=MLPConfig.max_steps)",
   "id": "efed6ca46233328d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Inference",
   "id": "af0c069cfb0e2b6f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "18b3ab05935f1d51"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
