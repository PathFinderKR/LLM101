{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Assignment 2\n",
    "\n",
    "In this assignment, you will continue with the Bigram Language Model from the Lecture. Make the training loop and inference for the model."
   ],
   "id": "1d8158dfddb2ac72"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Importing Libraries",
   "id": "c011cbf15834af26"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-02-09T10:53:47.923400Z",
     "start_time": "2025-02-09T10:53:46.935839Z"
    }
   },
   "source": [
    "import os\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from src.utils import load_text, set_seed"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Configuration",
   "id": "4a4a0b5274f4dca2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T10:53:47.927884Z",
     "start_time": "2025-02-09T10:53:47.924557Z"
    }
   },
   "cell_type": "code",
   "source": [
    "@dataclass\n",
    "class BigramConfig:\n",
    "    root_dir: str = os.getcwd() + \"/../../\"\n",
    "    dataset_path: str = \"data/names.txt\"\n",
    "\n",
    "    # Tokenizer\n",
    "    vocab_size: int = 0  # Set later\n",
    "\n",
    "    seed: int = 101"
   ],
   "id": "ef1253f6800c36c1",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Reproducibility",
   "id": "5726a09b7e375389"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T10:53:47.945144Z",
     "start_time": "2025-02-09T10:53:47.928804Z"
    }
   },
   "cell_type": "code",
   "source": "set_seed(BigramConfig.seed)",
   "id": "5e4d38445588cdbb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed set to 101\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Dataset",
   "id": "d74be7e01434a14b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T10:53:47.953598Z",
     "start_time": "2025-02-09T10:53:47.946370Z"
    }
   },
   "cell_type": "code",
   "source": "names = load_text(BigramConfig.root_dir + BigramConfig.dataset_path).splitlines()",
   "id": "32e86ea6f15f11b8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded text data from /mnt/c/Users/cheir/GitHub/LLM101/notebooks/Assignments/../../data/names.txt (length: 228145 characters).\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Preprocessing",
   "id": "3aff6f7e1bbade4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T10:53:47.959001Z",
     "start_time": "2025-02-09T10:53:47.954467Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Add special token\n",
    "names = [\".\" + name + \".\" for name in names]"
   ],
   "id": "ed24c0db0ce9da98",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Tokenizer",
   "id": "f9e38655c11342dd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T10:53:47.966214Z",
     "start_time": "2025-02-09T10:53:47.959835Z"
    }
   },
   "cell_type": "code",
   "source": [
    "chars = [chr(i) for i in range(97, 123)]  # all alphabet characters\n",
    "chars.insert(0, \".\")  # Add special token\n",
    "BigramConfig.vocab_size = len(chars)\n",
    "str2idx = {char: idx for idx, char in enumerate(chars)}\n",
    "idx2str = {idx: char for char, idx in str2idx.items()}"
   ],
   "id": "9a2f768e619dfb02",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Model",
   "id": "91e4e1dda633584d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T10:53:47.973789Z",
     "start_time": "2025-02-09T10:53:47.967151Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initialize weights\n",
    "W = torch.randn(BigramConfig.vocab_size, BigramConfig.vocab_size, requires_grad=True)\n",
    "b = torch.randn(BigramConfig.vocab_size, requires_grad=True)\n",
    "params = [W, b]"
   ],
   "id": "523dd8edb6b3e9c8",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Training",
   "id": "51ca7979aafb68a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Task 1: Train Bigram Language Model (Neural Network Approach)\n",
    "\n",
    "Make the training loop for the Bigram Language Model."
   ],
   "id": "bd6125c93c9c7519"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T10:53:48.030403Z",
     "start_time": "2025-02-09T10:53:47.974720Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Set of Input, Target pairs\n",
    "inputs, targets = [], []\n",
    "for name in names:\n",
    "    for char1, char2 in zip(name, name[1:]):\n",
    "        input = str2idx[char1]\n",
    "        target = str2idx[char2]\n",
    "        inputs.append(input)\n",
    "        targets.append(target)\n",
    "\n",
    "# Convert to tensor\n",
    "inputs = torch.tensor(inputs, dtype=torch.long)\n",
    "targets = torch.tensor(targets, dtype=torch.long)"
   ],
   "id": "f8f5165e72e37f1b",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T10:53:48.034740Z",
     "start_time": "2025-02-09T10:53:48.031565Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(f\"Number of Input, Target pairs: {len(inputs)}\")\n",
    "print(f\"Input shape: {inputs.shape}\")\n",
    "print(f\"Target shape: {targets.shape}\")\n",
    "print(f\"First (Input, Target): ({inputs[0]}, {targets[0]})\")\n",
    "print(f\"Second (Input, Target): ({inputs[1]}, {targets[1]})\")"
   ],
   "id": "ebe57ba03f098d90",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Input, Target pairs: 228146\n",
      "Input shape: torch.Size([228146])\n",
      "Target shape: torch.Size([228146])\n",
      "First (Input, Target): (0, 5)\n",
      "Second (Input, Target): (5, 13)\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T10:53:48.052639Z",
     "start_time": "2025-02-09T10:53:48.036043Z"
    }
   },
   "cell_type": "code",
   "source": [
    "################################################################################\n",
    "# TODO:                                                                        #\n",
    "# One-hot encode the input tensor.                                             #\n",
    "################################################################################\n",
    "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "inputs_encoded = F.one_hot(inputs, num_classes=BigramConfig.vocab_size)\n",
    "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "# Convert data type to float\n",
    "inputs_encoded = inputs_encoded.float()"
   ],
   "id": "4baab50fd5515e6a",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T10:53:54.502944Z",
     "start_time": "2025-02-09T10:53:48.053924Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Training Loop\n",
    "steps = 100\n",
    "lr = 10\n",
    "\n",
    "for step in range(1, steps + 1):\n",
    "    # Forward pass\n",
    "    ################################################################################\n",
    "    # TODO:                                                                        #\n",
    "    # Implement the forward pass.                                                  #\n",
    "    ################################################################################\n",
    "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    logits = inputs_encoded @ W + b\n",
    "    probs = logits.exp() / logits.exp().sum(dim=-1, keepdim=True)\n",
    "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    \n",
    "    # loss\n",
    "    log_probs = torch.log(probs + 1e-9)  # Add small value to prevent log(0)\n",
    "    loss = -log_probs[torch.arange(len(targets)), targets].mean()\n",
    "    \n",
    "    # Backward pass\n",
    "    ################################################################################\n",
    "    # TODO:                                                                        #\n",
    "    # Implement the backward pass.                                                 #\n",
    "    ################################################################################\n",
    "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    for param in params:\n",
    "        param.grad = None\n",
    "    loss.backward()\n",
    "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    \n",
    "    # Update weights\n",
    "    ################################################################################\n",
    "    # TODO:                                                                        #\n",
    "    # Update the weights of the model using the gradients.                         #\n",
    "    ################################################################################\n",
    "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    for param in params:\n",
    "        param.data = param.data - lr * param.grad\n",
    "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    \n",
    "    if step % 10 == 0:\n",
    "        print(f\"Step {step}, Loss {loss.item():.4f}\")"
   ],
   "id": "1a5ef07b0b820e5a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 10, Loss 2.8777\n",
      "Step 20, Loss 2.7156\n",
      "Step 30, Loss 2.6413\n",
      "Step 40, Loss 2.5989\n",
      "Step 50, Loss 2.5718\n",
      "Step 60, Loss 2.5532\n",
      "Step 70, Loss 2.5397\n",
      "Step 80, Loss 2.5294\n",
      "Step 90, Loss 2.5214\n",
      "Step 100, Loss 2.5150\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Inference",
   "id": "4d956fef0a027963"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Task 2: Generate a Name\n",
    "\n",
    "Create a function to generate a name using the trained Bigram Language Model."
   ],
   "id": "786f852b486b92cb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T10:53:54.510153Z",
     "start_time": "2025-02-09T10:53:54.504088Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create a function to generate a name\n",
    "def generate_name():\n",
    "    new_name = []\n",
    "    start_idx = str2idx[\".\"]\n",
    "    \n",
    "    while True:\n",
    "        ################################################################################\n",
    "        # TODO:                                                                        #\n",
    "        # 1. Forward pass                                                              #\n",
    "        # 2. Sample the next token                                                     #\n",
    "        # 3. Decode the token                                                          #\n",
    "        # 4. Update the start_idx                                                      #\n",
    "        # 5. Break if the next character is \".\"                                        #\n",
    "        ################################################################################\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        # Forward pass\n",
    "        x_encoded = F.one_hot(torch.tensor([start_idx]), num_classes=BigramConfig.vocab_size).float()\n",
    "        logits = torch.matmul(x_encoded, W) + b\n",
    "        prob = logits.exp() / logits.exp().sum(dim=-1, keepdim=True)\n",
    "        \n",
    "        # Sample\n",
    "        next_idx = torch.multinomial(prob, num_samples=1).item()\n",
    "        \n",
    "        # Decode\n",
    "        new_char = idx2str[next_idx]\n",
    "        new_name.append(new_char)\n",
    "        \n",
    "        # Update\n",
    "        start_idx = next_idx\n",
    "        \n",
    "        # Break if \".\"\n",
    "        if next_idx == str2idx[\".\"]:\n",
    "            break\n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    return ''.join(new_name)\n",
    "\n",
    "# Generate 5 names\n",
    "for _ in range(5):\n",
    "    print(generate_name())"
   ],
   "id": "b31dfacd08b51cd3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aymienzeta.\n",
      "dhchatt.\n",
      "rneara.\n",
      "gn.\n",
      "todeleanion.\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Extra Credit\n",
    "\n",
    "We have already made our own custom auto-grad Tensor class. Let's use it!\n",
    "\n",
    "Train the Bigram Language Model using our custom auto-grad Tensor class.\n",
    "\n",
    "**Do not use any built-in PyTorch functions.** (other deep learning libraries are also prohibited)"
   ],
   "id": "11d00faaddbe4b44"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T10:53:54.517495Z",
     "start_time": "2025-02-09T10:53:54.511212Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Tensor:\n",
    "    def __init__(self, data, _children=(), _operation=''):\n",
    "        self.data = data\n",
    "        self._prev = set(_children)\n",
    "        self.gradient = 0\n",
    "        self._backward = lambda: None\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"tensor=({self.data})\"\n",
    "\n",
    "    def __add__(self, other):  # self + other\n",
    "        output = Tensor(self.data + other.data, (self, other), '+')\n",
    "        def _backward():\n",
    "            self.gradient = 1 * output.gradient\n",
    "            other.gradient = 1 * output.gradient\n",
    "        output._backward = _backward\n",
    "        return output\n",
    "\n",
    "    def __mul__(self, other):  # self * other\n",
    "        output = Tensor(self.data * other.data, (self, other), '*')\n",
    "        def _backward():\n",
    "            self.gradient = other.data * output.gradient\n",
    "            other.gradient = self.data * output.gradient\n",
    "        output._backward = _backward\n",
    "        return output\n",
    "\n",
    "    def tanh(self):  # tanh(self)\n",
    "        output = Tensor(math.tanh(self.data), (self,), 'tanh')\n",
    "        def _backward():\n",
    "            self.gradient = (1.0 - math.tanh(self.data) ** 2) * output.gradient\n",
    "        output._backward = _backward\n",
    "        return output\n",
    "\n",
    "    def __pow__(self, power):  # self ** power\n",
    "        assert isinstance(power, (int, float)), \"Power must be an int or a float\"\n",
    "        output = Tensor(self.data ** power, (self,), f'**{power}')\n",
    "        def _backward():\n",
    "            self.gradient = power * (self.data ** (power - 1)) * output.gradient\n",
    "        output._backward = _backward\n",
    "        return output\n",
    "\n",
    "    def backward(self):\n",
    "        topo = []\n",
    "        visited = set()\n",
    "        def build_topo(v):\n",
    "            if v not in visited:\n",
    "                visited.add(v)\n",
    "                for child in v._prev:\n",
    "                    build_topo(child)\n",
    "                topo.append(v)\n",
    "        build_topo(self)\n",
    "        self.gradient = 1\n",
    "        for node in reversed(topo):\n",
    "            node._backward()\n",
    "\n",
    "    def __neg__(self): # -self\n",
    "        return self * Tensor(-1.0)\n",
    "\n",
    "    def __sub__(self, other): # self - other\n",
    "        return self + (-other)"
   ],
   "id": "e6f26efc1212bb48",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T10:53:54.527085Z",
     "start_time": "2025-02-09T10:53:54.518536Z"
    }
   },
   "cell_type": "code",
   "source": [
    "################################################################################\n",
    "# TODO:                                                                        #\n",
    "################################################################################\n",
    "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****"
   ],
   "id": "e3b7815ada66df8c",
   "outputs": [],
   "execution_count": 14
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
