{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Assignment 2",
   "id": "1d8158dfddb2ac72"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Task 1: Train Bigram Language Model (Neural Network Approach)\n",
    "\n",
    "Let's continue with the Bigram Language Model from the lecture and finish the training loop."
   ],
   "id": "3b18f121d3812f4c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Importing Libraries",
   "id": "c011cbf15834af26"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-02-09T07:00:44.745647Z",
     "start_time": "2025-02-09T07:00:43.312251Z"
    }
   },
   "source": [
    "import os\n",
    "import math\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from dataclasses import dataclass\n",
    "from src.utils import load_text, set_seed"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Configuration",
   "id": "4a4a0b5274f4dca2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T07:00:44.749908Z",
     "start_time": "2025-02-09T07:00:44.746995Z"
    }
   },
   "cell_type": "code",
   "source": [
    "@dataclass\n",
    "class BigramConfig:\n",
    "    root_dir: str = os.getcwd() + \"/../../\"\n",
    "    dataset_path: str = \"data/names.txt\"\n",
    "\n",
    "    # Tokenizer\n",
    "    vocab_size: int = 0  # Set later\n",
    "\n",
    "    seed: int = 101"
   ],
   "id": "ef1253f6800c36c1",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Reproducibility",
   "id": "5726a09b7e375389"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T07:00:44.772170Z",
     "start_time": "2025-02-09T07:00:44.750694Z"
    }
   },
   "cell_type": "code",
   "source": "set_seed(BigramConfig.seed)",
   "id": "5e4d38445588cdbb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed set to 101\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Dataset",
   "id": "d74be7e01434a14b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T07:00:44.781629Z",
     "start_time": "2025-02-09T07:00:44.773851Z"
    }
   },
   "cell_type": "code",
   "source": "names = load_text(BigramConfig.root_dir + BigramConfig.dataset_path).splitlines()",
   "id": "32e86ea6f15f11b8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded text data from /mnt/c/Users/cheir/GitHub/LLM101/notebooks/Assignments/../../data/names.txt (length: 228145 characters).\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Tokenizer",
   "id": "f9e38655c11342dd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T07:00:44.784981Z",
     "start_time": "2025-02-09T07:00:44.782565Z"
    }
   },
   "cell_type": "code",
   "source": [
    "chars = [chr(i) for i in range(97, 123)]  # all alphabet characters\n",
    "chars.insert(0, \".\")  # Add special token\n",
    "BigramConfig.vocab_size = len(chars)\n",
    "str2idx = {char: idx for idx, char in enumerate(chars)}\n",
    "idx2str = {idx: char for char, idx in str2idx.items()}"
   ],
   "id": "9a2f768e619dfb02",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Model",
   "id": "91e4e1dda633584d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T07:00:44.792055Z",
     "start_time": "2025-02-09T07:00:44.785933Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initialize weights\n",
    "W = torch.randn(BigramConfig.vocab_size, BigramConfig.vocab_size)\n",
    "b = torch.randn(BigramConfig.vocab_size)\n",
    "\n",
    "params = [W, b]\n",
    "for param in params:\n",
    "    param.requires_grad = True"
   ],
   "id": "523dd8edb6b3e9c8",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Training",
   "id": "51ca7979aafb68a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T07:00:44.836152Z",
     "start_time": "2025-02-09T07:00:44.792951Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Set of Input, Target pairs\n",
    "inputs, targets = [], []\n",
    "for name in names:\n",
    "    for char1, char2 in zip(name, name[1:]):\n",
    "        input = str2idx[char1]\n",
    "        target = str2idx[char2]\n",
    "        inputs.append(input)\n",
    "        targets.append(target)\n",
    "\n",
    "# Convert to tensor\n",
    "inputs = torch.tensor(inputs, dtype=torch.long)\n",
    "targets = torch.tensor(targets, dtype=torch.long)"
   ],
   "id": "f8f5165e72e37f1b",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T07:00:44.861072Z",
     "start_time": "2025-02-09T07:00:44.837260Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# One-hot encoding\n",
    "# ------------------\n",
    "# Write your implementation here.\n",
    "inputs_encoded = F.one_hot(inputs, num_classes=BigramConfig.vocab_size)\n",
    "targets_encoded = F.one_hot(targets, num_classes=BigramConfig.vocab_size)\n",
    "# ------------------\n",
    "\n",
    "# Convert data type to float\n",
    "inputs_encoded = inputs_encoded.float()\n",
    "targets_encoded = targets_encoded.float()"
   ],
   "id": "4baab50fd5515e6a",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T07:00:47.724613Z",
     "start_time": "2025-02-09T07:00:44.862337Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Training Loop\n",
    "steps = 100\n",
    "lr = 0.01\n",
    "\n",
    "for step in range(steps):\n",
    "    # ------------------\n",
    "    # Write your implementation here.\n",
    "    # Forward pass\n",
    "    logits = inputs_encoded @ W + b\n",
    "    \n",
    "    # loss\n",
    "    loss = F.cross_entropy(logits, targets)\n",
    "    \n",
    "    # Backward pass\n",
    "    for param in params:\n",
    "        param.grad = None\n",
    "    loss.backward()\n",
    "    \n",
    "    # Update weights\n",
    "    for param in params:\n",
    "        param.data = param.data - lr * param.grad\n",
    "    # ------------------\n",
    "    \n",
    "    print(f\"Step: {step+1}, Loss: {loss.item()}\")"
   ],
   "id": "1a5ef07b0b820e5a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 1, Loss: 4.384030818939209\n",
      "Step: 2, Loss: 4.382874965667725\n",
      "Step: 3, Loss: 4.381721496582031\n",
      "Step: 4, Loss: 4.380569934844971\n",
      "Step: 5, Loss: 4.37941837310791\n",
      "Step: 6, Loss: 4.378269195556641\n",
      "Step: 7, Loss: 4.3771209716796875\n",
      "Step: 8, Loss: 4.375974178314209\n",
      "Step: 9, Loss: 4.374828815460205\n",
      "Step: 10, Loss: 4.373684883117676\n",
      "Step: 11, Loss: 4.372542381286621\n",
      "Step: 12, Loss: 4.371401309967041\n",
      "Step: 13, Loss: 4.370262145996094\n",
      "Step: 14, Loss: 4.369123458862305\n",
      "Step: 15, Loss: 4.367987632751465\n",
      "Step: 16, Loss: 4.366851806640625\n",
      "Step: 17, Loss: 4.36571741104126\n",
      "Step: 18, Loss: 4.364584922790527\n",
      "Step: 19, Loss: 4.363454341888428\n",
      "Step: 20, Loss: 4.362324237823486\n",
      "Step: 21, Loss: 4.3611955642700195\n",
      "Step: 22, Loss: 4.3600687980651855\n",
      "Step: 23, Loss: 4.35894250869751\n",
      "Step: 24, Loss: 4.357818603515625\n",
      "Step: 25, Loss: 4.356695652008057\n",
      "Step: 26, Loss: 4.355574131011963\n",
      "Step: 27, Loss: 4.35445499420166\n",
      "Step: 28, Loss: 4.353335857391357\n",
      "Step: 29, Loss: 4.3522186279296875\n",
      "Step: 30, Loss: 4.351102352142334\n",
      "Step: 31, Loss: 4.349987030029297\n",
      "Step: 32, Loss: 4.348874568939209\n",
      "Step: 33, Loss: 4.347762584686279\n",
      "Step: 34, Loss: 4.346651554107666\n",
      "Step: 35, Loss: 4.345542907714844\n",
      "Step: 36, Loss: 4.34443473815918\n",
      "Step: 37, Loss: 4.34332799911499\n",
      "Step: 38, Loss: 4.342222690582275\n",
      "Step: 39, Loss: 4.341119289398193\n",
      "Step: 40, Loss: 4.340016841888428\n",
      "Step: 41, Loss: 4.338915824890137\n",
      "Step: 42, Loss: 4.33781623840332\n",
      "Step: 43, Loss: 4.33671760559082\n",
      "Step: 44, Loss: 4.335620403289795\n",
      "Step: 45, Loss: 4.334524631500244\n",
      "Step: 46, Loss: 4.33342981338501\n",
      "Step: 47, Loss: 4.332336902618408\n",
      "Step: 48, Loss: 4.331245422363281\n",
      "Step: 49, Loss: 4.3301544189453125\n",
      "Step: 50, Loss: 4.329065322875977\n",
      "Step: 51, Loss: 4.327978134155273\n",
      "Step: 52, Loss: 4.3268914222717285\n",
      "Step: 53, Loss: 4.325806140899658\n",
      "Step: 54, Loss: 4.3247222900390625\n",
      "Step: 55, Loss: 4.323639392852783\n",
      "Step: 56, Loss: 4.32255744934082\n",
      "Step: 57, Loss: 4.321477890014648\n",
      "Step: 58, Loss: 4.320399284362793\n",
      "Step: 59, Loss: 4.319322109222412\n",
      "Step: 60, Loss: 4.318245887756348\n",
      "Step: 61, Loss: 4.317171573638916\n",
      "Step: 62, Loss: 4.316098213195801\n",
      "Step: 63, Loss: 4.315025329589844\n",
      "Step: 64, Loss: 4.313955307006836\n",
      "Step: 65, Loss: 4.312885284423828\n",
      "Step: 66, Loss: 4.311817169189453\n",
      "Step: 67, Loss: 4.310749530792236\n",
      "Step: 68, Loss: 4.309683799743652\n",
      "Step: 69, Loss: 4.308619499206543\n",
      "Step: 70, Loss: 4.307556629180908\n",
      "Step: 71, Loss: 4.306493759155273\n",
      "Step: 72, Loss: 4.30543327331543\n",
      "Step: 73, Loss: 4.304373741149902\n",
      "Step: 74, Loss: 4.30331563949585\n",
      "Step: 75, Loss: 4.3022589683532715\n",
      "Step: 76, Loss: 4.30120325088501\n",
      "Step: 77, Loss: 4.3001484870910645\n",
      "Step: 78, Loss: 4.299095153808594\n",
      "Step: 79, Loss: 4.298043727874756\n",
      "Step: 80, Loss: 4.296992301940918\n",
      "Step: 81, Loss: 4.295943737030029\n",
      "Step: 82, Loss: 4.294895172119141\n",
      "Step: 83, Loss: 4.293848037719727\n",
      "Step: 84, Loss: 4.292802333831787\n",
      "Step: 85, Loss: 4.2917585372924805\n",
      "Step: 86, Loss: 4.290715217590332\n",
      "Step: 87, Loss: 4.289673328399658\n",
      "Step: 88, Loss: 4.288632392883301\n",
      "Step: 89, Loss: 4.28759241104126\n",
      "Step: 90, Loss: 4.286554336547852\n",
      "Step: 91, Loss: 4.285516738891602\n",
      "Step: 92, Loss: 4.284481525421143\n",
      "Step: 93, Loss: 4.283447265625\n",
      "Step: 94, Loss: 4.282413482666016\n",
      "Step: 95, Loss: 4.281381607055664\n",
      "Step: 96, Loss: 4.280350208282471\n",
      "Step: 97, Loss: 4.279321193695068\n",
      "Step: 98, Loss: 4.278292179107666\n",
      "Step: 99, Loss: 4.2772650718688965\n",
      "Step: 100, Loss: 4.276238918304443\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Inference",
   "id": "4d956fef0a027963"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T07:00:47.881625Z",
     "start_time": "2025-02-09T07:00:47.725996Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create a function to generate a name\n",
    "def generate_name():\n",
    "    new_name = []\n",
    "    start_idx = str2idx[\".\"]\n",
    "    \n",
    "    while True:\n",
    "        # Forward pass\n",
    "        logits = torch.matmul(inputs_encoded[start_idx], W) + b\n",
    "        probs = F.softmax(logits, dim=0)\n",
    "        \n",
    "        # Sample\n",
    "        next_idx = torch.multinomial(probs[start_idx], num_samples=1).item()\n",
    "        \n",
    "        # Decode\n",
    "        new_char = idx2str[next_idx]\n",
    "        new_name.append(new_char)\n",
    "        \n",
    "        # Update\n",
    "        start_idx = next_idx\n",
    "        \n",
    "        if start_idx == str2idx[\".\"]:\n",
    "            break\n",
    "            \n",
    "    return ''.join(new_name)\n",
    "\n",
    "# Generate 5 names\n",
    "for _ in range(5):\n",
    "    print(generate_name())"
   ],
   "id": "b31dfacd08b51cd3",
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "prob_dist must be 1 or 2 dim",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[10], line 28\u001B[0m\n\u001B[1;32m     26\u001B[0m \u001B[38;5;66;03m# Generate 5 names\u001B[39;00m\n\u001B[1;32m     27\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m _ \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m5\u001B[39m):\n\u001B[0;32m---> 28\u001B[0m     \u001B[38;5;28mprint\u001B[39m(generate_name())\n",
      "Cell \u001B[0;32mIn[10], line 12\u001B[0m, in \u001B[0;36mgenerate_name\u001B[0;34m()\u001B[0m\n\u001B[1;32m      9\u001B[0m probs \u001B[38;5;241m=\u001B[39m F\u001B[38;5;241m.\u001B[39msoftmax(logits, dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m)\n\u001B[1;32m     11\u001B[0m \u001B[38;5;66;03m# Sample\u001B[39;00m\n\u001B[0;32m---> 12\u001B[0m next_idx \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mmultinomial(probs[start_idx], num_samples\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\u001B[38;5;241m.\u001B[39mitem()\n\u001B[1;32m     14\u001B[0m \u001B[38;5;66;03m# Decode\u001B[39;00m\n\u001B[1;32m     15\u001B[0m new_char \u001B[38;5;241m=\u001B[39m idx2str[next_idx]\n",
      "\u001B[0;31mRuntimeError\u001B[0m: prob_dist must be 1 or 2 dim"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Task 2: Mini-batch Training\n",
    "\n",
    "In practice, datasets are too large to fit into the memory. Therefore, we use mini-batch training.\n",
    "\n",
    "Implement mini-batch training for the Bigram Language Model."
   ],
   "id": "5a79b8ed4a4c4215"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Create a function to generate mini-batches\n",
    "def get_batches(xs, ys, batch_size):\n",
    "    # ------------------\n",
    "    # Write your implementation here.\n",
    "    pass\n",
    "\n",
    "    # ------------------"
   ],
   "id": "9c29f20c2fb4e2a2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Training Loop\n",
    "steps = 100\n",
    "lr = 0.01\n",
    "batch_size = 64\n",
    "\n",
    "for step in range(steps):\n",
    "    # ------------------\n",
    "    # Write your implementation here.\n",
    "    pass\n",
    "    # ------------------\n",
    "    \n",
    "    print(f\"Step: {step}, Loss: {loss.item()}\")"
   ],
   "id": "fb4e96a0a1e88f94",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Extra Credit\n",
    "\n",
    "We have already made our own custom auto-grad Tensor class. Let's use it!\n",
    "\n",
    "Train the Bigram Language Model using our custom auto-grad Tensor class.\n",
    "\n",
    "**Do not use any built-in PyTorch functions.** (other deep learning libraries are also prohibited)"
   ],
   "id": "11d00faaddbe4b44"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class Tensor:\n",
    "    def __init__(self, data, _children=(), _operation=''):\n",
    "        self.data = data\n",
    "        self._prev = set(_children)\n",
    "        self.gradient = 0\n",
    "        self._backward = lambda: None\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"tensor=({self.data})\"\n",
    "\n",
    "    def __add__(self, other):  # self + other\n",
    "        output = Tensor(self.data + other.data, (self, other), '+')\n",
    "        def _backward():\n",
    "            self.gradient = 1 * output.gradient\n",
    "            other.gradient = 1 * output.gradient\n",
    "        output._backward = _backward\n",
    "        return output\n",
    "\n",
    "    def __mul__(self, other):  # self * other\n",
    "        output = Tensor(self.data * other.data, (self, other), '*')\n",
    "        def _backward():\n",
    "            self.gradient = other.data * output.gradient\n",
    "            other.gradient = self.data * output.gradient\n",
    "        output._backward = _backward\n",
    "        return output\n",
    "\n",
    "    def tanh(self):  # tanh(self)\n",
    "        output = Tensor(math.tanh(self.data), (self,), 'tanh')\n",
    "        def _backward():\n",
    "            self.gradient = (1.0 - math.tanh(self.data) ** 2) * output.gradient\n",
    "        output._backward = _backward\n",
    "        return output\n",
    "\n",
    "    def __pow__(self, power):  # self ** power\n",
    "        assert isinstance(power, (int, float)), \"Power must be an int or a float\"\n",
    "        output = Tensor(self.data ** power, (self,), f'**{power}')\n",
    "        def _backward():\n",
    "            self.gradient = power * (self.data ** (power - 1)) * output.gradient\n",
    "        output._backward = _backward\n",
    "        return output\n",
    "\n",
    "    def backward(self):\n",
    "        topo = []\n",
    "        visited = set()\n",
    "        def build_topo(v):\n",
    "            if v not in visited:\n",
    "                visited.add(v)\n",
    "                for child in v._prev:\n",
    "                    build_topo(child)\n",
    "                topo.append(v)\n",
    "        build_topo(self)\n",
    "        self.gradient = 1\n",
    "        for node in reversed(topo):\n",
    "            node._backward()\n",
    "\n",
    "    def __neg__(self): # -self\n",
    "        return self * Tensor(-1.0)\n",
    "\n",
    "    def __sub__(self, other): # self - other\n",
    "        return self + (-other)"
   ],
   "id": "e6f26efc1212bb48",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ------------------\n",
    "# Write your implementation here.\n",
    "\n",
    "# ------------------"
   ],
   "id": "e3b7815ada66df8c",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
