{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Assignment 2",
   "id": "1d8158dfddb2ac72"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Task 1: Train Bigram Language Model (Machine Learning Approach)",
   "id": "3b18f121d3812f4c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Importing Libraries",
   "id": "c011cbf15834af26"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from dataclasses import dataclass\n",
    "from src.utils import load_text, set_seed"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Configuration",
   "id": "4a4a0b5274f4dca2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "@dataclass\n",
    "class BigramConfig:\n",
    "    root_dir: str = os.getcwd() + \"/../../\"\n",
    "    dataset_path: str = \"data/raw/names.txt\"\n",
    "\n",
    "    # Tokenizer\n",
    "    vocab_size: int = 0  # Set later\n",
    "\n",
    "    seed: int = 101"
   ],
   "id": "ef1253f6800c36c1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Reproducibility",
   "id": "5726a09b7e375389"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "set_seed(BigramConfig.seed)",
   "id": "5e4d38445588cdbb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Dataset",
   "id": "d74be7e01434a14b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "names = load_text(BigramConfig.root_dir + BigramConfig.dataset_path).splitlines()",
   "id": "32e86ea6f15f11b8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Tokenizer",
   "id": "f9e38655c11342dd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "chars = [chr(i) for i in range(97, 123)]  # all alphabet characters\n",
    "chars.insert(0, \".\")  # Add special token\n",
    "BigramConfig.vocab_size = len(chars)\n",
    "str2idx = {char: idx for idx, char in enumerate(chars)}\n",
    "idx2str = {idx: char for char, idx in str2idx.items()}"
   ],
   "id": "9a2f768e619dfb02"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Model",
   "id": "91e4e1dda633584d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "W = torch.randn(BigramConfig.vocab_size, BigramConfig.vocab_size)\n",
    "b = torch.randn(BigramConfig.vocab_size)"
   ],
   "id": "523dd8edb6b3e9c8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Training",
   "id": "51ca7979aafb68a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "xs, ys = [], []\n",
    "\n",
    "for name in names:\n",
    "    for char1, char2 in zip(name, name[1:]):\n",
    "        x = str2idx[char1]\n",
    "        y = str2idx[char2]\n",
    "        xs.append(x)\n",
    "        ys.append(y)\n",
    "        \n",
    "xs = torch.tensor(xs, dtype=torch.long)\n",
    "ys = torch.tensor(ys, dtype=torch.long)"
   ],
   "id": "f8f5165e72e37f1b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# One-hot encoding\n",
    "# ------------------\n",
    "# Write your implementation here.\n",
    "\n",
    "# ------------------"
   ],
   "id": "4baab50fd5515e6a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Training Loop\n",
    "steps = 10\n",
    "lr = 0.01\n",
    "for step in range(steps):\n",
    "    # ------------------\n",
    "    # Write your implementation here.\n",
    "    # Forward pass\n",
    "    # Calculate loss\n",
    "    # Backward pass\n",
    "    # Update weights\n",
    "    # ------------------\n",
    "    pass"
   ],
   "id": "1a5ef07b0b820e5a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Inference",
   "id": "4d956fef0a027963"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Create a function to generate a name\n",
    "\n",
    "def generate_name():\n",
    "    # ------------------\n",
    "    # Write your implementation here.\n",
    "    pass\n",
    "    # ------------------\n",
    "\n",
    "# Generate 5 names\n",
    "for _ in range(5):\n",
    "    print(generate_name())"
   ],
   "id": "b31dfacd08b51cd3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Task 2: Mini-batch Training\n",
    "\n",
    "In practice, datasets are too large to fit in memory. Therefore, we use mini-batch training.\n",
    "\n",
    "Implement mini-batch training for the Bigram Language Model."
   ],
   "id": "5a79b8ed4a4c4215"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Create a function to generate mini-batches\n",
    "def get_batches(xs, ys, batch_size):\n",
    "    # ------------------\n",
    "    # Write your implementation here.\n",
    "    pass\n",
    "\n",
    "    # ------------------"
   ],
   "id": "9c29f20c2fb4e2a2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Extra Credit\n",
    "\n",
    "We have already made our own custom auto-grad Tensor class. Let's use it!\n",
    "\n",
    "Train the Bigram Language Model using our custom auto-grad Tensor class.\n",
    "\n",
    "**Do not use any built-in PyTorch functions.** (other deep learning libraries are also prohibited)"
   ],
   "id": "11d00faaddbe4b44"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class Tensor:\n",
    "    def __init__(self, data, _children=(), _operation='', label=''):\n",
    "        self.data = data\n",
    "        self._prev = set(_children)  # _children: tensors that lead to this tensor (ex: 2 * 3 = 6, 2 and 3 are children of 6)\n",
    "        self._operation = _operation  # _operation: operation that lead to this tensor (ex: 2 * 3 = 6, * is the operation)\n",
    "        self.label = label  # label: name of the tensor\n",
    "        self.gradient = 0\n",
    "        self._backward = lambda: None\n",
    "\n",
    "    # method to print the tensor\n",
    "    def __repr__(self):\n",
    "        return f\"data=({self.data})\"\n",
    "\n",
    "    # method to add two tensors\n",
    "    def __add__(self, other):\n",
    "        output = Tensor(self.data + other.data, (self, other), '+')\n",
    "        def _backward():\n",
    "            # ------------------\n",
    "            # Write your implementation here.\n",
    "            # f = self + other\n",
    "            # df/dself = 1\n",
    "            # df/dother = 1\n",
    "            self.gradient += 1 * output.gradient  # d(self + other)/dself = 1\n",
    "            other.gradient += 1 * output.gradient  # d(self + other)/dother = 1\n",
    "            # ------------------\n",
    "        output._backward = _backward\n",
    "        return output\n",
    "\n",
    "    # method to multiply two tensors\n",
    "    def __mul__(self, other):\n",
    "        output = Tensor(self.data * other.data, (self, other), '*')\n",
    "        def _backward():\n",
    "            # ------------------\n",
    "            # Write your implementation here.\n",
    "            self.gradient += other.data * output.gradient  # d(self * other)/dself = other\n",
    "            other.gradient += self.data * output.gradient  # d(self * other)/dother = self\n",
    "            # ------------------\n",
    "        output._backward = _backward\n",
    "        return output\n",
    "\n",
    "    # tanh: activation function\n",
    "    def tanh(self):\n",
    "        output = Tensor(math.tanh(self.data), (self,), 'tanh')\n",
    "        def _backward():\n",
    "            # ------------------\n",
    "            # Write your implementation here.\n",
    "            self.gradient += (1.0 - math.tanh(self.data) ** 2) * output.gradient  # d(tanh(x))/dx = 1 - tanh(x)^2\n",
    "            # ------------------\n",
    "        output._backward = _backward\n",
    "        return output\n",
    "\n",
    "    def __pow__(self, power):  # self ** power\n",
    "        assert isinstance(power, (int, float)), \"Power must be an int or a float\"\n",
    "        output = Tensor(self.data ** power, (self,), f'**{power}')\n",
    "        def _backward():\n",
    "            # ------------------\n",
    "            # Write your implementation here.\n",
    "            self.gradient += power * (self.data ** (power - 1)) * output.gradient  # d(x^p)/dx = p * x^(p-1)\n",
    "            # ------------------\n",
    "        output._backward = _backward\n",
    "        return output\n",
    "\n",
    "    # method to calculate the gradient\n",
    "    def backward(self):\n",
    "        topo = []\n",
    "        visited = set()\n",
    "        def build_topo(v):\n",
    "            if v not in visited:\n",
    "                visited.add(v)\n",
    "                for child in v._prev:\n",
    "                    build_topo(child)\n",
    "                topo.append(v)\n",
    "        build_topo(self)\n",
    "\n",
    "        self.gradient = 1\n",
    "        for node in reversed(topo):\n",
    "            node._backward()\n",
    "\n",
    "    def __neg__(self): # -self\n",
    "        return self * Tensor(-1.0)\n",
    "\n",
    "    def __sub__(self, other): # self - other\n",
    "        return self + (-other)"
   ],
   "id": "e6f26efc1212bb48"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
