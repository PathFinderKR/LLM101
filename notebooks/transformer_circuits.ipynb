{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# A Mathematical Framework for Transformer Circuits",
   "id": "95fa6d03e25cf399"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Importing libraries",
   "id": "98c18e9341dff696"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from dataclasses import dataclass\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import wandb\n",
    "from src.utils import load_text, set_seed, configure_device\n",
    "from src.tokenizer import CharTokenizer, BPETokenizer\n",
    "from src.train import split_text, TextDataset, setup_optimizer, setup_scheduler, train_steps"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Configuration",
   "id": "dc19c9912571d6ef"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "@dataclass\n",
    "class GPTConfig:\n",
    "    root_dir: str = os.getcwd() + \"/../\"\n",
    "    dataset_path: str = 'data/raw/shakespeare.txt'\n",
    "    device: torch.device = torch.device('cpu')  # Automatic device configuration\n",
    "\n",
    "    # wandb\n",
    "    project: str = \"LLM101-Scaling-Laws\"\n",
    "\n",
    "    # Tokenizer\n",
    "    tokenizer: str = \"char\"  # char or bpe\n",
    "\n",
    "    # Model\n",
    "    context_size: int = 4\n",
    "    n_layer: int = 2\n",
    "    n_head: int = 2\n",
    "    d_embed: int = 128\n",
    "    d_ff: int = 512\n",
    "    dropout: float = 0.2\n",
    "    flash_attention: bool = False\n",
    "\n",
    "    # Training\n",
    "    val_size: float = 0.05\n",
    "    max_steps: int = 1000\n",
    "    val_interval: int = 500\n",
    "    batch_size: int = 64\n",
    "    optimizer: str = \"AdamW\"  # AdamW or SGD\n",
    "    learning_rate: float = 0.001\n",
    "    weight_decay: float = 0.01\n",
    "    scheduler: str = \"cosine\"  # cosine or linear\n",
    "    warmup_ratio: float = 0.1\n",
    "    grad_clip: float = 1.0\n",
    "    mixed_precision: bool = False\n",
    "    seed: int = 101"
   ],
   "id": "6f308e981a72112c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Weights & Biases",
   "id": "7cf3e4d040598736"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "wandb.login()",
   "id": "860637b4789e505"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Reproducibility",
   "id": "a4fde2b9605b2b6c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "6dbfa51f0b950a7f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Device",
   "id": "dfd0a8949f9d42a4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "196fc9cee3f92697"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Tokenizer",
   "id": "becf38394d1a7c1a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "6f5a4c9448e05c2e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Dataset",
   "id": "9d72031756bc8243"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "2a4838507a327bfa"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Model",
   "id": "55bec1e5754c102b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class CasualSelfAttention(nn.Module):\n",
    "    def __init__(self, d_embed: int, n_head: int, dropout: float):\n",
    "        super(CasualSelfAttention, self).__init__()\n",
    "        self.n_head = n_head\n",
    "        self.d_head = d_embed // n_head\n",
    "        self.scale = self.d_head ** -0.5\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.query = nn.Linear(d_embed, d_embed, bias=False)\n",
    "        self.key = nn.Linear(d_embed, d_embed, bias=False)\n",
    "        self.value = nn.Linear(d_embed, d_embed, bias=False)\n",
    "        self.out = nn.Linear(d_embed, d_embed, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, context_size, _ = x.size()\n",
    "\n",
    "        # Query, Key, Value\n",
    "        q = self.query(x)  # (batch_size, context_size, d_embed)\n",
    "        k = self.key(x)  # (batch_size, context_size, d_embed)\n",
    "        v = self.value(x)  # (batch_size, context_size, d_embed)\n",
    "        q = q.view(batch_size, context_size, self.n_head, self.d_head).transpose(1, 2)  # (batch_size, n_head, context_size, d_head)\n",
    "        k = k.view(batch_size, context_size, self.n_head, self.d_head).transpose(1, 2)  # (batch_size, n_head, context_size, d_head)\n",
    "        v = v.view(batch_size, context_size, self.n_head, self.d_head).transpose(1, 2)  # (batch_size, n_head, context_size, d_head)\n",
    "\n",
    "        # Scaled Dot-Product Attention\n",
    "        attn_scores = torch.matmul(q, k.transpose(-2, -1)) * self.scale  # (batch_size, n_head, context_size, context_size)\n",
    "\n",
    "        # Masking\n",
    "        mask = torch.triu(torch.ones(context_size, context_size, device=x.device), diagonal=1).bool()  # (context_size, context_size)\n",
    "        attn_scores = attn_scores.masked_fill(mask[None, None, :, :], float('-inf'))  # (batch_size, n_head, context_size, context_size)\n",
    "\n",
    "        # Softmax\n",
    "        attn_scores = F.softmax(attn_scores, dim=-1)  # (batch_size, n_head, context_size, context_size)\n",
    "\n",
    "        # Dropout\n",
    "        attn_scores = F.dropout(attn_scores, p=self.dropout, training=self.training)  # (batch_size, n_head, context_size, context_size)\n",
    "\n",
    "        # Weighted Sum\n",
    "        attn_output = torch.matmul(attn_scores, v)  # (batch_size, n_head, context_size, d_head)\n",
    "\n",
    "        # Concatenation\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, context_size, self.n_head * self.d_head)  # (batch_size, context_size, d_embed)\n",
    "\n",
    "        # Output Linear Layer\n",
    "        x = self.out(attn_output)  # (batch_size, context_size, d_embed)\n",
    "        return x\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_embed: int, d_ff: int, dropout: float):\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.fc1 = nn.Linear(d_embed, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_embed)\n",
    "        self.activation = nn.GELU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_embed: int, n_head: int, d_ff: int, dropout: float):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.self_attention = CasualSelfAttention(d_embed, n_head, dropout)\n",
    "        self.layer_norm1 = nn.LayerNorm(d_embed)\n",
    "        self.feed_forward = FeedForward(d_embed, d_ff, dropout)\n",
    "        self.layer_norm2 = nn.LayerNorm(d_embed)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.self_attention(self.layer_norm1(x))\n",
    "        x = x + self.feed_forward(self.layer_norm2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class GPT(nn.Module):\n",
    "    def __init__(self, config: GPTConfig):\n",
    "        super(GPT, self).__init__()\n",
    "        self.config = config\n",
    "        self.token_embedding = nn.Embedding(config.vocab_size, config.d_embed)\n",
    "        self.positional_embedding = nn.Embedding(config.context_size, config.d_embed)\n",
    "        self.layers = nn.ModuleList([DecoderLayer(config.d_embed, config.n_head, config.d_ff, config.dropout) for _ in range(config.n_layer)])\n",
    "        self.layer_norm = nn.LayerNorm(config.d_embed)\n",
    "        self.linear = nn.Linear(config.d_embed, config.vocab_size, bias=False)\n",
    "\n",
    "    def forward(self, x):  # x: (batch_size, context_size)\n",
    "        batch_size, context_size = x.size()\n",
    "        assert context_size <= self.config.context_size, \\\n",
    "            f\"context_size should be less than or equal to {self.config.context_size}\"\n",
    "\n",
    "        # Embedding\n",
    "        token_embed = self.token_embedding(x)  # (batch_size, context_size, d_embed)\n",
    "        pos_idx = torch.arange(context_size, device=x.device)  # (context_size)\n",
    "        pos_embed = self.positional_embedding(pos_idx)  # (batch_size, context_size, d_embed)\n",
    "        x = token_embed + pos_embed  # (batch_size, context_size, d_embed)\n",
    "\n",
    "        # Decoder layers\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)  # (batch_size, context_size, d_embed)\n",
    "\n",
    "        # Output\n",
    "        x = self.layer_norm(x)\n",
    "        x = self.linear(x)  # (batch_size, context_size, vocab_size)\n",
    "        return x\n",
    "\n",
    "    def loss(self, logits, target):\n",
    "        logits = logits.view(-1, self.config.vocab_size)  # (batch_size * context_size, vocab_size)\n",
    "        target = target.view(-1)  # (batch_size * context_size)\n",
    "        return F.cross_entropy(logits, target)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, tokenizer, prompt, max_new_tokens, device, temperature=1.0):\n",
    "        if temperature < 0.0 or temperature > 1.0:\n",
    "            raise ValueError(\"temperature must be between 0.0 and 1.0\")\n",
    "\n",
    "        self.eval()\n",
    "        print(prompt)\n",
    "\n",
    "        # Encode\n",
    "        x = tokenizer.encode(prompt).to(device).unsqueeze(0)  # (batch_size=1, prompt_size)\n",
    "\n",
    "        # Generation loop\n",
    "        for _ in range(max_new_tokens):\n",
    "            # Truncate\n",
    "            context = x[:, -self.config.context_size:]  # (batch_size=1, context_size)\n",
    "\n",
    "            # Forward\n",
    "            logits = self.forward(context)[:, -1, :] / temperature  # (batch_size=1, vocab_size)\n",
    "\n",
    "            # Sample\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)  # (batch_size=1, 1)\n",
    "\n",
    "            # Concatenate\n",
    "            x = torch.cat((x, next_token), dim=-1)  # (batch_size=1, context_size + 1)\n",
    "\n",
    "            # Decode\n",
    "            text = tokenizer.decode([next_token[0].item()])\n",
    "            print(text, end='', flush=True)\n"
   ],
   "id": "9794cff02d9cb6b8"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
