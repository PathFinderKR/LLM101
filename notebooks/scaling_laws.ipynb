{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# MNIST",
   "id": "5a2879c29388ed3d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Importing libraries",
   "id": "2d50a27f6d5cce3c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import umap\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataclasses import dataclass\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import wandb"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Configuration",
   "id": "98c4d28cd3cad2ed"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    debug: bool = True\n",
    "    dataset_path: str = \"data/\"\n",
    "    device: torch.device = torch.device('cpu')  # Automatic device configuration\n",
    "    \n",
    "    # Model\n",
    "    d_embed: int = 16\n",
    "    \n",
    "    # Training\n",
    "    batch_size: int = 100\n",
    "    max_steps: int = 600  # Total number of training samples = max_steps * batch_size = 60,000\n",
    "    lr: float = 0.01\n",
    "\n",
    "    seed: int = 101\n",
    "    \n",
    "config = Config()"
   ],
   "id": "e963e5d4ec007f5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Weights & Biases",
   "id": "a31cf9cf4900855f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "if not args.debug:\n",
    "    wandb.login(key=os.environ.get(\"WANDB_API_KEY\"))"
   ],
   "id": "6121518ce41a4c1e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Reproducibility",
   "id": "5f6db2e5a4d1e7f2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def set_seed(seed: int):\n",
    "    \"\"\"\n",
    "    Set the random seed for reproducibility.\n",
    "\n",
    "    Args:\n",
    "        seed (int): The seed value to set.\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    print(f\"Random seed set to {seed}\")\n",
    "    \n",
    "set_seed(config.seed)"
   ],
   "id": "191c93f9a466321d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Device",
   "id": "278e9b144ce43909"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def configure_device() -> torch.device:\n",
    "    \"\"\"\n",
    "    Configure the device for training.\n",
    "\n",
    "    Returns:\n",
    "        torch.device: The device to use for training.\n",
    "    \"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        num_gpu = torch.cuda.device_count()\n",
    "        print(f\"Running on {num_gpu} {torch.cuda.get_device_name()} GPU(s)\")\n",
    "    elif torch.backends.mps.is_available():\n",
    "        device = torch.device(\"mps\")\n",
    "        print(f\"Running on {device}\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(f\"Running on {device}\")\n",
    "    return device\n",
    "\n",
    "config.device = configure_device()"
   ],
   "id": "1a99637921af915b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Dataset",
   "id": "3ca2487805337c7c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "mnist_train = datasets.MNIST(\n",
    "    root=config.dataset_path,\n",
    "    train=True,\n",
    "    transform=transforms.ToTensor(),\n",
    "    download=True,\n",
    ")\n",
    "mnist_test = datasets.MNIST(\n",
    "    root=config.dataset_path,\n",
    "    train=False,\n",
    "    transform=transforms.ToTensor(),\n",
    "    download=True,\n",
    ")\n",
    "train_loader = DataLoader(\n",
    "    dataset=mnist_train,\n",
    "    batch_size=config.batch_size,\n",
    "    shuffle=True,\n",
    "    drop_last=True\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    dataset=mnist_test,\n",
    "    batch_size=10000,\n",
    "    shuffle=False,\n",
    "    drop_last=False\n",
    ")"
   ],
   "id": "f36d41edc5fb262b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Model",
   "id": "6048021bf6d4344a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, d_embed):\n",
    "        super(MLP, self).__init__()\n",
    "        self.linear1 = nn.Linear(784, d_embed)\n",
    "        self.linear2 = nn.Linear(d_embed, 10)\n",
    "\n",
    "    def forward(self, x):  # x: (batch_size, 1, 28, 28)\n",
    "        x = x.view(x.size(0), -1)  # (batch_size, 784)\n",
    "        x = self.linear1(x)  # (batch_size, d_embed)\n",
    "        x = F.relu(x)\n",
    "        x = self.linear2(x)  # (batch_size, 10)\n",
    "        return x"
   ],
   "id": "a403b849ab70133b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Training",
   "id": "ff3993ec24776d76"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def train_epoch(model: nn.Module, dataloader: DataLoader, optimizer: Optimizer, scheduler: LambdaLR, current_epoch: int, total_epochs: int, grad_clip: float, device: torch.device, wandb_run: wandb.sdk.wandb_run.Run) -> nn.Module:\n",
    "    \"\"\"\n",
    "    Train the model for one epoch.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The model to train.\n",
    "        dataloader (DataLoader): DataLoader for the training data.\n",
    "        optimizer (Optimizer): Optimizer to use.\n",
    "        scheduler (lr_scheduler): Learning rate scheduler.\n",
    "        current_epoch (int): Current epoch number.\n",
    "        total_epochs (int): Total number of epochs.\n",
    "        grad_clip (float): Gradient clipping value.\n",
    "        device (torch.device): Device to run the model on.\n",
    "        wandb_run (wandb.sdk.wandb_run.Run): Wandb run for logging.\n",
    "\n",
    "    Returns:\n",
    "        model (nn.Module): The trained model.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    progress_bar = tqdm(enumerate(dataloader), total=len(dataloader), desc=f\"Epoch {current_epoch+1}/{total_epochs}\")\n",
    "    wandb_run.watch(model, log=\"all\", log_freq=len(dataloader))\n",
    "\n",
    "    for batch_idx, (inputs, targets) in progress_bar:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(inputs)\n",
    "        loss = model.loss(logits, targets)\n",
    "        loss.backward()\n",
    "        clip_grad_norm_(model.parameters(), grad_clip)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        running_loss += loss.item()\n",
    "        progress_bar.set_postfix(loss=f\"{running_loss / (batch_idx + 1):.4f}\")\n",
    "\n",
    "        if wandb_run is not None:\n",
    "            wandb_run.log(\n",
    "                {\"Train Loss\": loss.item(),\n",
    "                \"Learning Rate\": optimizer.param_groups[0]['lr']},\n",
    "                step=current_epoch * len(dataloader) + batch_idx\n",
    "            )\n",
    "\n",
    "    progress_bar.close()\n",
    "    print(f\"Epoch {current_epoch+1}/{total_epochs} Loss: {running_loss / len(dataloader):.4f}\")\n",
    "\n",
    "    return model"
   ],
   "id": "e94734619a69cadd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Evaluation",
   "id": "2432d4dfb5c6dd08"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def evaluate(model: nn.Module, dataloader: DataLoader, device: torch.device, wandb_run: wandb.sdk.wandb_run.Run) -> float:\n",
    "    \"\"\"\n",
    "    Evaluate the model on the validation set.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The model to evaluate.\n",
    "        dataloader (DataLoader): DataLoader for the validation data.\n",
    "        device (torch.device): Device to run the model on.\n",
    "        wandb_run (wandb.sdk.wandb_run.Run): Wandb run for logging.\n",
    "\n",
    "    Returns:\n",
    "        loss (float): The average loss on the validation set.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    progress_bar = tqdm(enumerate(dataloader), total=len(dataloader), desc=\"Validation\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in progress_bar:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            logits = model(inputs)\n",
    "            loss = model.loss(logits, targets)\n",
    "            running_loss += loss.item()\n",
    "            progress_bar.set_postfix(loss=f\"{running_loss / (batch_idx + 1):.4f}\")\n",
    "\n",
    "    progress_bar.close()\n",
    "\n",
    "    avg_loss = running_loss / len(dataloader)\n",
    "\n",
    "    if wandb_run is not None:\n",
    "        wandb_run.log({\n",
    "            \"Validation Loss\": avg_loss,\n",
    "        })\n",
    "\n",
    "    print(f\"Validation Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    return avg_loss"
   ],
   "id": "77b18ed41df114ec"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Scaling laws",
   "id": "59ae2ec0274ec25b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def plot_scaling_laws(x, y, x_label, y_label, title, wandb_run):\n",
    "    \"\"\"\n",
    "    Plot the given data in log-log scale and log the figure to wandb.\n",
    "    \"\"\"\n",
    "    from scipy.stats import linregress\n",
    "\n",
    "    # Convert to log-space\n",
    "    x_log = np.log10(x)\n",
    "    y_log = np.log10(y)\n",
    "    slope, intercept, r_value, p_value, std_err = linregress(x_log, y_log)\n",
    "\n",
    "    # Create figure and axis\n",
    "    fig, ax = plt.subplots(figsize=(6, 5))\n",
    "    ax.scatter(x, y, label='Data', alpha=0.7)\n",
    "\n",
    "    # Best fit line in log space\n",
    "    fit_line_x = np.linspace(x_log.min(), x_log.max(), 100)\n",
    "    fit_line_y = slope * fit_line_x + intercept\n",
    "    ax.plot(10 ** fit_line_x, 10 ** fit_line_y, 'r--',\n",
    "            label=f'Fit: y = {10 ** intercept:.2f} * x^{slope:.2f}')\n",
    "    ax.set_xscale('log')\n",
    "    ax.set_yscale('log')\n",
    "    ax.set_xlabel(x_label)\n",
    "    ax.set_ylabel(y_label)\n",
    "    ax.set_title(title)\n",
    "    ax.legend()\n",
    "\n",
    "    # Log the figure to wandb\n",
    "    if wandb_run is not None:\n",
    "        wandb.log({title: wandb.Image(fig)})\n",
    "    plt.close(fig)"
   ],
   "id": "5f93c63679053c2d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Compute",
   "id": "b3411c8c22d9bfac"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def compute_experiment(\n",
    "        model_sizes: dict, dataset_sizes: dict,\n",
    "        train_text: str, val_text: str, tokenizer: CharTokenizer | BPETokenizer,\n",
    "        optimizer_name: str, lr: float, weight_decay: float, scheduler_type: str, warmup_ratio: float,\n",
    "        grad_clip: float, device: torch.device, project: str, root_dir: str):\n",
    "    \"\"\"\n",
    "    Compute vs test loss scaling laws.\n",
    "\n",
    "    Args:\n",
    "        model_sizes (dict): Dictionary with the model sizes.\n",
    "        dataset_sizes (dict): Dictionary with the dataset sizes.\n",
    "        train_text (str): Text data for training.\n",
    "        val_text (str): Text data for validation.\n",
    "        tokenizer (CharTokenizer | BPETokenizer): Tokenizer instance.\n",
    "        optimizer_name (str): Name of the optimizer.\n",
    "        lr (float): Learning rate.\n",
    "        weight_decay (float): Weight decay.\n",
    "        scheduler_type (str): Type of the scheduler.\n",
    "        warmup_ratio (float): Ratio of the warmup steps.\n",
    "        grad_clip (float): Gradient clipping value.\n",
    "        device (torch.device): Device for training.\n",
    "        project (str): Name of the project.\n",
    "        root_dir (str): Root directory of the project.\n",
    "    \"\"\"\n",
    "    compute_values = []\n",
    "    test_losses = []\n",
    "    exp = 1\n",
    "\n",
    "    for model_size in model_sizes:\n",
    "        for dataset_size in dataset_sizes:\n",
    "            wandb_run = wandb.init(\n",
    "                project=project,\n",
    "                name=f\"Compute vs Test Loss - exp {exp}\",\n",
    "                dir=root_dir\n",
    "            )\n",
    "            print(f\"Wandb run initialized: {wandb_run.id}\")\n",
    "\n",
    "            # Subset the training data\n",
    "            subset_train_text = train_text[:int(len(train_text) * dataset_sizes[dataset_size])]\n",
    "            train_dataset = TextDataset(text=subset_train_text, tokenizer=tokenizer, context_size=model_sizes[model_size][\"context_size\"])\n",
    "            val_dataset = TextDataset(text=val_text, tokenizer=tokenizer, context_size=model_sizes[model_size][\"context_size\"])\n",
    "            if model_size == \"small\":\n",
    "                batch_size = 512\n",
    "            elif model_size == \"medium\":\n",
    "                batch_size = 128\n",
    "            elif model_size == \"large\":\n",
    "                batch_size = 64\n",
    "            elif model_size == \"xl\":\n",
    "                batch_size = 32\n",
    "            else:\n",
    "                batch_size = 128\n",
    "            train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "            val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "            print(f\"Number of tokens: {len(train_dataset)}\")\n",
    "\n",
    "            # Initialize the model\n",
    "            model = GPT(GPTConfig(\n",
    "                vocab_size=tokenizer.vocab_size,\n",
    "                context_size=model_sizes[model_size][\"context_size\"],\n",
    "                n_layer=model_sizes[model_size][\"n_layer\"],\n",
    "                n_head=model_sizes[model_size][\"n_head\"],\n",
    "                d_embed=model_sizes[model_size][\"d_embed\"],\n",
    "                d_ff=model_sizes[model_size][\"d_ff\"],\n",
    "                dropout=model_sizes[model_size][\"dropout\"]\n",
    "            )).to(device)\n",
    "            num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "            # FLOPs\n",
    "            flops = 2 * num_params * len(train_dataset)\n",
    "            print(f\"FLOPs: {flops}\")\n",
    "\n",
    "            # Initialize the optimizer and scheduler\n",
    "            optimizer = setup_optimizer(\n",
    "                model=model,\n",
    "                optimizer_name=optimizer_name,\n",
    "                lr=lr,\n",
    "                weight_decay=weight_decay\n",
    "            )\n",
    "            scheduler = setup_scheduler(\n",
    "                optimizer=optimizer,\n",
    "                scheduler_type=scheduler_type,\n",
    "                warmup_ratio=warmup_ratio,\n",
    "                total_steps=len(train_loader) * 1\n",
    "            )\n",
    "\n",
    "            # Train the model for one epoch\n",
    "            model = train_epoch(\n",
    "                model=model,\n",
    "                dataloader=train_loader,\n",
    "                optimizer=optimizer,\n",
    "                scheduler=scheduler,\n",
    "                current_epoch=0,\n",
    "                total_epochs=1,\n",
    "                grad_clip=grad_clip,\n",
    "                device=device,\n",
    "                wandb_run=wandb_run\n",
    "            )\n",
    "            test_loss = evaluate(\n",
    "                model=model,\n",
    "                dataloader=val_loader,\n",
    "                device=device,\n",
    "                wandb_run=wandb_run\n",
    "            )\n",
    "\n",
    "            compute_values.append(flops)\n",
    "            test_losses.append(test_loss)\n",
    "            wandb_run.finish()\n",
    "            exp += 1\n",
    "\n",
    "    plot_scaling_laws(\n",
    "        x=compute_values,\n",
    "        y=test_losses,\n",
    "        x_label=\"Compute (FLOPs)\",\n",
    "        y_label=\"Test Loss\",\n",
    "        title=\"Compute vs Test Loss\",\n",
    "        wandb_run=wandb_run\n",
    "    )"
   ],
   "id": "7a3911e53a50a8d6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Dataset size",
   "id": "f75b0ba5a94488a8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def dataset_size_experiment(\n",
    "        dataset_sizes: dict, model_size: dict,\n",
    "        train_text: str, val_text: str, tokenizer: CharTokenizer | BPETokenizer, batch_size: int,\n",
    "        optimizer_name: str, lr: float, weight_decay: float, scheduler_type: str, warmup_ratio: float,\n",
    "        grad_clip: float, device: torch.device, project: str, root_dir: str):\n",
    "    \"\"\"\n",
    "    Dataset size vs test loss scaling laws.\n",
    "\n",
    "    Args:\n",
    "        dataset_sizes (dict): Dictionary with the dataset sizes.\n",
    "        model_size (dict): Dictionary with the model size.\n",
    "        train_text (str): Text data for training.\n",
    "        val_text (str): Text data for validation.\n",
    "        tokenizer (CharTokenizer | BPETokenizer): Tokenizer instance.\n",
    "        batch_size (int): Batch size the DataLoaders.\n",
    "        optimizer_name (str): Name of the optimizer.\n",
    "        lr (float): Learning rate.\n",
    "        weight_decay (float): Weight decay.\n",
    "        scheduler_type (str): Type of the scheduler.\n",
    "        warmup_ratio (float): Ratio of the warmup steps.\n",
    "        grad_clip (float): Gradient clipping value.\n",
    "        device (torch.device): Device for training.\n",
    "        project (str): Name of the project.\n",
    "        root_dir (str): Root directory of the project.\n",
    "    \"\"\"\n",
    "    num_tokens = []\n",
    "    test_losses = []\n",
    "    exp = 1\n",
    "\n",
    "    for dataset_size in dataset_sizes:\n",
    "        wandb_run = wandb.init(\n",
    "            project=project,\n",
    "            name=f\"Dataset Size vs Test Loss - exp {exp}\",\n",
    "            dir=root_dir\n",
    "        )\n",
    "        print(f\"Wandb run initialized: {wandb_run.id}\")\n",
    "\n",
    "        subset_train_text = train_text[:int(len(train_text) * dataset_sizes[dataset_size])]\n",
    "        train_dataset = TextDataset(text=subset_train_text, tokenizer=tokenizer, context_size=model_size[\"context_size\"])\n",
    "        val_dataset = TextDataset(text=val_text, tokenizer=tokenizer, context_size=model_size[\"context_size\"])\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "        print(f\"Number of tokens: {len(train_dataset)}\")\n",
    "\n",
    "        # Initialize the model\n",
    "        model = GPT(GPTConfig(\n",
    "            vocab_size=tokenizer.vocab_size,\n",
    "            context_size=model_size[\"context_size\"],\n",
    "            n_layer=model_size[\"n_layer\"],\n",
    "            n_head=model_size[\"n_head\"],\n",
    "            d_embed=model_size[\"d_embed\"],\n",
    "            d_ff=model_size[\"d_ff\"],\n",
    "            dropout=model_size[\"dropout\"]\n",
    "        )).to(device)\n",
    "\n",
    "        # Initialize the optimizer and scheduler\n",
    "        optimizer = setup_optimizer(\n",
    "            model=model,\n",
    "            optimizer_name=optimizer_name,\n",
    "            lr=lr,\n",
    "            weight_decay=weight_decay\n",
    "        )\n",
    "        scheduler = setup_scheduler(\n",
    "            optimizer=optimizer,\n",
    "            scheduler_type=scheduler_type,\n",
    "            warmup_ratio=warmup_ratio,\n",
    "            total_steps=len(train_loader) * 1\n",
    "        )\n",
    "\n",
    "        # Train the model for one epoch\n",
    "        model = train_epoch(\n",
    "            model=model,\n",
    "            dataloader=train_loader,\n",
    "            optimizer=optimizer,\n",
    "            scheduler=scheduler,\n",
    "            current_epoch=0,\n",
    "            total_epochs=1,\n",
    "            grad_clip=grad_clip,\n",
    "            device=device,\n",
    "            wandb_run=wandb_run\n",
    "        )\n",
    "        test_loss = evaluate(\n",
    "            model=model,\n",
    "            dataloader=val_loader,\n",
    "            device=device,\n",
    "            wandb_run=wandb_run\n",
    "        )\n",
    "\n",
    "        num_tokens.append(len(train_dataset))\n",
    "        test_losses.append(test_loss)\n",
    "        wandb_run.finish()\n",
    "        exp += 1\n",
    "\n",
    "    plot_scaling_laws(\n",
    "        x=num_tokens,\n",
    "        y=test_losses,\n",
    "        x_label=\"Dataset Size\",\n",
    "        y_label=\"Test Loss\",\n",
    "        title=\"Dataset Size vs Test Loss\",\n",
    "        wandb_run=wandb_run\n",
    "    )"
   ],
   "id": "6a324056018aa32c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Model size",
   "id": "fa6056fe1d8f7409"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def model_size_experiment(\n",
    "        model_sizes: dict, dataset_size: float,\n",
    "        train_text: str, val_text: str, tokenizer: CharTokenizer | BPETokenizer,\n",
    "        optimizer_name: str, lr: float, weight_decay: float, scheduler_type: str, warmup_ratio: float,\n",
    "        grad_clip: float, device: torch.device, project: str, root_dir: str):\n",
    "    \"\"\"\n",
    "    Model size vs test loss scaling laws.\n",
    "\n",
    "    Args:\n",
    "        model_sizes (dict): Dictionary with the model sizes.\n",
    "        dataset_size (float): Dictionary with the dataset sizes.\n",
    "        train_text (str): Text data for training.\n",
    "        val_text (str): Text data for validation.\n",
    "        tokenizer (CharTokenizer | BPETokenizer): Tokenizer instance.\n",
    "        optimizer_name (str): Name of the optimizer.\n",
    "        lr (float): Learning rate.\n",
    "        weight_decay (float): Weight decay.\n",
    "        scheduler_type (str): Type of the scheduler.\n",
    "        warmup_ratio (float): Ratio of the warmup steps.\n",
    "        grad_clip (float): Gradient clipping value.\n",
    "        device (torch.device): Device for training.\n",
    "        project (str): Name of the project.\n",
    "        root_dir (str): Root directory of the project.\n",
    "    \"\"\"\n",
    "    parameters = []\n",
    "    test_losses = []\n",
    "    exp = 1\n",
    "\n",
    "    for model_size in model_sizes:\n",
    "        wandb_run = wandb.init(\n",
    "            project=project,\n",
    "            name=f\"Parameters vs Test Loss - exp {exp}\",\n",
    "            dir=root_dir\n",
    "        )\n",
    "        print(f\"Wandb run initialized: {wandb_run.id}\")\n",
    "\n",
    "        subset_train_text = train_text[:int(len(train_text) * dataset_size)]\n",
    "        train_dataset = TextDataset(text=subset_train_text, tokenizer=tokenizer, context_size=model_sizes[model_size][\"context_size\"])\n",
    "        val_dataset = TextDataset(text=val_text, tokenizer=tokenizer, context_size=model_sizes[model_size][\"context_size\"])\n",
    "        if model_size == \"small\":\n",
    "            batch_size = 512\n",
    "        elif model_size == \"medium\":\n",
    "            batch_size = 128\n",
    "        elif model_size == \"large\":\n",
    "            batch_size = 64\n",
    "        elif model_size == \"xl\":\n",
    "            batch_size = 32\n",
    "        else:\n",
    "            batch_size = 128\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "\n",
    "        # Initialize the model\n",
    "        model = GPT(GPTConfig(\n",
    "            vocab_size=tokenizer.vocab_size,\n",
    "            context_size=model_sizes[model_size][\"context_size\"],\n",
    "            n_layer=model_sizes[model_size][\"n_layer\"],\n",
    "            n_head=model_sizes[model_size][\"n_head\"],\n",
    "            d_embed=model_sizes[model_size][\"d_embed\"],\n",
    "            d_ff=model_sizes[model_size][\"d_ff\"],\n",
    "            dropout=model_sizes[model_size][\"dropout\"]\n",
    "        )).to(device)\n",
    "        num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "        print(f\"Number of parameters: {num_params}\")\n",
    "\n",
    "        # Initialize the optimizer and scheduler\n",
    "        optimizer = setup_optimizer(\n",
    "            model=model,\n",
    "            optimizer_name=optimizer_name,\n",
    "            lr=lr,\n",
    "            weight_decay=weight_decay\n",
    "        )\n",
    "        scheduler = setup_scheduler(\n",
    "            optimizer=optimizer,\n",
    "            scheduler_type=scheduler_type,\n",
    "            warmup_ratio=warmup_ratio,\n",
    "            total_steps=len(train_loader) * 1\n",
    "        )\n",
    "\n",
    "\n",
    "        # Train the model for one epoch\n",
    "        model = train_epoch(\n",
    "            model=model,\n",
    "            dataloader=train_loader,\n",
    "            optimizer=optimizer,\n",
    "            scheduler=scheduler,\n",
    "            current_epoch=0,\n",
    "            total_epochs=1,\n",
    "            grad_clip=grad_clip,\n",
    "            device=device,\n",
    "            wandb_run=wandb_run\n",
    "        )\n",
    "        test_loss = evaluate(\n",
    "            model=model,\n",
    "            dataloader=val_loader,\n",
    "            device=device,\n",
    "            wandb_run=wandb_run\n",
    "        )\n",
    "\n",
    "        parameters.append(num_params)\n",
    "        test_losses.append(test_loss)\n",
    "        wandb_run.finish()\n",
    "        exp += 1\n",
    "\n",
    "    plot_scaling_laws(\n",
    "        x=parameters,\n",
    "        y=test_losses,\n",
    "        x_label=\"Number of Layers\",\n",
    "        y_label=\"Test Loss\",\n",
    "        title=\"Model Size vs Test Loss\",\n",
    "        wandb_run=wandb_run\n",
    "    )"
   ],
   "id": "29c1ff58495ab685"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### UMAP",
   "id": "83812f4c882ca1de"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "a3b18eb89c2bc263"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
